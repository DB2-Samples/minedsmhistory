{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the required lib\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "'''\n",
    "To get query condition string from reading the file :singleDBCondition\n",
    "and handle it to gain all parameters by split function\n",
    "'''\n",
    "DB_CONN_ID = None\n",
    "TBSP_NAME = None\n",
    "END_TIME = None\n",
    "INTERVAL = 0\n",
    "START_TIME = None\n",
    "REPORT_TYPE = None\n",
    "ISOK_ALL_PARA = 1\n",
    "GENERAL_ERROR = None\n",
    "\n",
    "def handle_query_condition():\n",
    "    # To declare below variables are global variables\n",
    "    global DB_CONN_ID\n",
    "    global TBSP_NAME\n",
    "    global END_TIME\n",
    "    global INTERVAL\n",
    "    global START_TIME\n",
    "    global REPORT_TYPE\n",
    "    global ISOK_ALL_PARA\n",
    "    global GENERAL_ERROR\n",
    "    \n",
    "    # To read parameters from singleDBCondition file\n",
    "    where_cause = !cat storageCondition\n",
    "    where_cause = re.search(r'DB_CONN_ID\\s?=\\s?.+END_TIME\\s?=\\s?.+INTERVAL\\s?=\\s?.+REPORT_TYPE\\s?=\\s?.+', where_cause[0])\n",
    "    GENERAL_ERROR = []\n",
    "    \n",
    "    if (where_cause <> None):\n",
    "        all_vars = where_cause.group().split(' ')\n",
    "        if (len(all_vars) <> 4) and (len(all_vars) <> 5):\n",
    "            err_str = 'The format of parameters is error.'\n",
    "            if err_str not in GENERAL_ERROR:\n",
    "                GENERAL_ERROR.append(err_str)\n",
    "            ISOK_ALL_PARA = 0\n",
    "        else:# len(all_vars) == 4 or 5\n",
    "            if (re.search(r'TBSP_NAME', all_vars[1]) <> None):\n",
    "                # To get the value of TBSP_NAME\n",
    "                if (re.search(r'TBSP_NAME\\s?=\\s?(.+)', all_vars[1]) == None):\n",
    "                    err_str = 'The format of TBSP_NAME error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    TBSP_NAME = re.search(r'TBSP_NAME\\s?=\\s?(.+)', all_vars[1]).group(1).upper()\n",
    "                       \n",
    "                # To get the value of REPORT_TYPE\n",
    "                if (re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[4]) == None):\n",
    "                    err_str = 'The format of REPORT_TYPE error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    REPORT_TYPE = re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[4]).group(1).upper()\n",
    "                    if (REPORT_TYPE != 'ALL' and REPORT_TYPE != 'TABLESPACE' and REPORT_TYPE != 'DATABASE' and REPORT_TYPE != 'TBSP_TABLE'):\n",
    "                        err_str = 'The format of REPORT_TYPE error.'\n",
    "                        if err_str not in GENERAL_ERROR:\n",
    "                            GENERAL_ERROR.append(err_str)\n",
    "                        ISOK_ALL_PARA = 0\n",
    "\n",
    "                # To get DB_CONN_ID\n",
    "                if (re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]) == None):\n",
    "                    err_str = 'The DB_CONN_ID can not be empty.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    DB_CONN_ID = re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]).group(1)\n",
    "                    if(REPORT_TYPE == 'DATABASE'):\n",
    "                        if DB_CONN_ID <> '*':\n",
    "                            err_str = 'The DB_CONN_ID must be *.'\n",
    "                            if err_str not in GENERAL_ERROR:\n",
    "                                GENERAL_ERROR.append(err_str)\n",
    "                            ISOK_ALL_PARA = 0\n",
    "\n",
    "                # To get the value of INTERVAL\n",
    "                if (re.search(r'INTERVAL\\s?=\\s?([0-9]+$)', all_vars[3]) == None):\n",
    "                    err_str = 'The format of INTERVAL error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    INTERVAL = int(re.search(r'(\\d+)', all_vars[3]).group())\n",
    "                    if (INTERVAL > 100):  # most get 100 data\n",
    "                        INTERVAL = 100\n",
    "\n",
    "                # To get START_TIME according to END_TIME\n",
    "                if (re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[2]) == None) | ( re.search(r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[2]) == None):\n",
    "                    err_str = 'The format of END_TIME error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    END_TIME = re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[2]).group() + ' ' + re.search( r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[2]).group()\n",
    "                    def is_valid_datetime(END_TIME):\n",
    "                        try:\n",
    "                            time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                            return True\n",
    "                        except:\n",
    "                            return False\n",
    "\n",
    "                    if is_valid_datetime(END_TIME) == True:\n",
    "                        datetime_tuple = time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                        # To slice datetime_tuple to gain exact time data\n",
    "                        year, month, day, hour, minute, second = datetime_tuple[:6]\n",
    "                        final_time = datetime.datetime(year, month, day, hour, minute, second) + datetime.timedelta(hours = -INTERVAL)\n",
    "                        # To transfer the datetime fields to string\n",
    "                        START_TIME = final_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    else:\n",
    "                        err_str = 'The format of END_TIME error.'\n",
    "                        if err_str not in GENERAL_ERROR:\n",
    "                            GENERAL_ERROR.append(err_str)\n",
    "                        ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                # To get the value of REPORT_TYPE\n",
    "                if (re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]) == None):\n",
    "                    err_str = 'The format of REPORT_TYPE error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    REPORT_TYPE = re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]).group(1).upper()\n",
    "                    if (REPORT_TYPE != 'ALL' and REPORT_TYPE != 'TABLESPACE' and REPORT_TYPE != 'DATABASE' and REPORT_TYPE != 'TBSP_TABLE'):\n",
    "                        err_str = 'The format of REPORT_TYPE error.'\n",
    "                        if err_str not in GENERAL_ERROR:\n",
    "                            GENERAL_ERROR.append(err_str)\n",
    "                        ISOK_ALL_PARA = 0\n",
    "\n",
    "                # To get DB_CONN_ID\n",
    "                if (re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]) == None):\n",
    "                    err_str = 'The DB_CONN_ID can not be empty.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    DB_CONN_ID = re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]).group(1)\n",
    "                    if(REPORT_TYPE == 'DATABASE'):\n",
    "                        if DB_CONN_ID <> '*':\n",
    "                            err_str = 'The DB_CONN_ID must be *.'\n",
    "                            if err_str not in GENERAL_ERROR:\n",
    "                                GENERAL_ERROR.append(err_str)\n",
    "                            ISOK_ALL_PARA = 0\n",
    "\n",
    "                # To get the value of INTERVAL\n",
    "                if (re.search(r'INTERVAL\\s?=\\s?([0-9]+$)', all_vars[2]) == None):\n",
    "                    err_str = 'The format of INTERVAL error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    INTERVAL = int(re.search(r'(\\d+)', all_vars[2]).group())\n",
    "                    if (INTERVAL > 100):  # most get 100 data\n",
    "                        INTERVAL = 100\n",
    "\n",
    "                # To get START_TIME according to END_TIME\n",
    "                if (re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]) == None) | ( re.search(r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]) == None):\n",
    "                    err_str = 'The format of END_TIME error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "                else:\n",
    "                    END_TIME = re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]).group() + ' ' + re.search( r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]).group()\n",
    "                    def is_valid_datetime(END_TIME):\n",
    "                        try:\n",
    "                            time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                            return True\n",
    "                        except:\n",
    "                            return False\n",
    "\n",
    "                    if is_valid_datetime(END_TIME) == True:\n",
    "                        datetime_tuple = time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                        # To slice datetime_tuple to gain exact time data\n",
    "                        year, month, day, hour, minute, second = datetime_tuple[:6]\n",
    "                        final_time = datetime.datetime(year, month, day, hour, minute, second) + datetime.timedelta(hours = -INTERVAL)\n",
    "                        # To transfer the datetime fields to string\n",
    "                        START_TIME = final_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    else:\n",
    "                        err_str = 'The format of END_TIME error.'\n",
    "                        if err_str not in GENERAL_ERROR:\n",
    "                            GENERAL_ERROR.append(err_str)\n",
    "                        ISOK_ALL_PARA = 0\n",
    "    else:\n",
    "        err_str = 'The format of parameters is error.'\n",
    "        if err_str not in GENERAL_ERROR:\n",
    "            GENERAL_ERROR.append(err_str)\n",
    "        ISOK_ALL_PARA = 0\n",
    "\n",
    "'''\n",
    "This function is to get all datatime string according to the result \n",
    "queried from database.\n",
    "\n",
    "The variable ori_datetime_str is a dictionary and its data \n",
    "from the combination of hour_list and date_all, which will \n",
    "be used for judging whether some data exists in it or not.\n",
    "'''\n",
    "def get_original_datatime_str(date_all, hour_list):\n",
    "    try:\n",
    "        ori_datetime_str = {}\n",
    "        for indx in range(len(hour_list)):\n",
    "            tmp_date_str = str(date_all[indx]).encode('unicode-escape').decode('string_escape')\n",
    "            tmp_hour_str = str(hour_list[indx])\n",
    "            tmp_datetime_str = ''\n",
    "            if(len(tmp_hour_str) == 1):#Change 1:00:00 into 01:00:00\n",
    "                tmp_hour_str = '0' + tmp_hour_str\n",
    "            tmp_datetime_str = tmp_date_str + ' ' + tmp_hour_str + ':00:00'\n",
    "            ori_datetime_str[tmp_datetime_str] = indx\n",
    "        return ori_datetime_str\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "#To store previous date string value as a reference data\n",
    "def format_x_axis(date_all, hour_list, x_ticks, x_ticks_lables):\n",
    "    try:\n",
    "        pre_date_str = str(date_all[0])\n",
    "\n",
    "        for dateIdx in range(len(date_all)):\n",
    "            # To get the data for the x-axis ticks\n",
    "            x_ticks.append(float('%0.1f' % dateIdx))\n",
    "            '''\n",
    "            Get the data for the lable of x-axis\n",
    "            If the label existed in the list x_ticks_lables,\n",
    "            put hour_str into x_ticks_lables \n",
    "            otherwise,put date_lables into x_ticks_lables\n",
    "            '''\n",
    "            hour_str = str(hour_list[dateIdx])\n",
    "            if len(hour_str) == 1:\n",
    "                hour_str = '0' + hour_str\n",
    "            date_str = str(date_all[dateIdx])\n",
    "            x_lables = date_str + ' ' + hour_str\n",
    "            if (dateIdx == 0):\n",
    "                x_ticks_lables.append(x_lables)\n",
    "            else:#dateIdx > 0\n",
    "                if (pre_date_str == date_str):\n",
    "                    x_ticks_lables.append(hour_str)\n",
    "                else:\n",
    "                    pre_date_str = date_str\n",
    "                    x_ticks_lables.append(x_lables)\n",
    "    except Exception as e:\n",
    "        return str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.interpolate import spline\n",
    "from sklearn import linear_model\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "\n",
    "#To print the error of query conditon, only print once\n",
    "for err_id in range(len(GENERAL_ERROR)):\n",
    "    print GENERAL_ERROR[err_id]\n",
    "    \n",
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'TBSP_TABLE')):\n",
    "    #To get data from the target database to assemble a data frame that will be used for the following graph\n",
    "    df_interval = %sql with temp as( select date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, tbsp_name, (decimal(tbsp_total_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, (decimal(tbsp_used_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 / 1024 as TBSP_USED_SIZE_MB, (decimal(tbsp_free_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 / 1024 as TBSP_FREE_SIZE_MB, (decimal(tbsp_usable_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 /1024 as TBSP_USABLE_SIZE_MB, case when tbsp_usable_pages > 0 then decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT from IBMIOCM.DB2LUW_MONTABLESPACE_HIS where IBMIOCM_DATABASE='{DB_CONN_ID}' and IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}' order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)), temp_stg as ( select row_number()over(partition by date, hour order by TBSP_USED_SIZE_MB desc) as row_id, date, hour, tbsp_name, TBSP_TOTAL_SIZE_MB, TBSP_USED_SIZE_MB, TBSP_FREE_SIZE_MB, TBSP_USABLE_SIZE_MB, TBSP_UTILIZATION_PERCENT from temp) select date, hour, tbsp_name, TBSP_TOTAL_SIZE_MB, TBSP_USED_SIZE_MB, TBSP_FREE_SIZE_MB, TBSP_USABLE_SIZE_MB, TBSP_UTILIZATION_PERCENT, row_id as rank from temp_stg where row_id <= 3; \n",
    "    #df_interval = %sql with temp as (select * from IBMIOCM.DB2LUW_MONTABLESPACE_HIS where IBMIOCM_DATABASE='{DB_CONN_ID}' and IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}'), temp_stg as (select date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, tbsp_name, (decimal(tbsp_total_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, (decimal(tbsp_used_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 / 1024 as TBSP_USED_SIZE_MB, (decimal(tbsp_free_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 / 1024 as TBSP_FREE_SIZE_MB, (decimal(tbsp_usable_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 /1024 as TBSP_USABLE_SIZE_MB, case when tbsp_usable_pages > 0 then decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT from temp order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)), temp_stg2 as (select row_number()over(partition by date, hour order by TBSP_USED_SIZE_MB desc) as row_id, date, hour, tbsp_name, TBSP_TOTAL_SIZE_MB, TBSP_USED_SIZE_MB, TBSP_FREE_SIZE_MB, TBSP_USABLE_SIZE_MB, TBSP_UTILIZATION_PERCENT from temp) select date, hour, tbsp_name, TBSP_TOTAL_SIZE_MB, TBSP_USED_SIZE_MB, TBSP_FREE_SIZE_MB, TBSP_USABLE_SIZE_MB, TBSP_UTILIZATION_PERCENT, row_id as rank from temp_stg where row_id <= 3;\n",
    "    if os.path.exists(\"tbsp.csv\"):\n",
    "        !rm tbsp.csv\n",
    "    if df_interval.empty:\n",
    "        print 'For Tablespace: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To judge whether file exists or not and if yes, delete it, if not save the data into a file storage.csv\n",
    "        df_interval.to_csv(\"tbsp.csv\", index_label = \"INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "\n",
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'TABLESPACE')):\n",
    "    #dataframe_week = %sql select year(collected) as year, week(collected) as week, substr(dbconn_id,1,30) as conn_name, date(min(collected)) as begin_date, date(max(collected)) as end_date, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(physical_reads_delta) as physical_reads, sum(act_completed_total_delta) as activities, sum(total_app_commits_delta) as commits, sum(total_act_time_delta) / (sum(act_aborted_total_delta) + sum(act_completed_total_delta)) as avg_activity_time_msec from ibm_dsm_views.throughput_all group by year(collected), week(collected), dbconn_id order by year(collected), week(collected), dbconn_id \n",
    "    df_interval = %sql select date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, tbsp_name, (tbsp_total_pages * tbsp_page_size) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, (tbsp_used_pages * tbsp_page_size) / 1024 / 1024 as TBSP_USED_SIZE_MB, (tbsp_free_pages * tbsp_page_size) / 1024 / 1024 as TBSP_FREE_SIZE_MB, (tbsp_usable_pages * tbsp_page_size) / 1024 /1024 as TBSP_USABLE_SIZE_MB, case when tbsp_usable_pages > 0 then decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT from IBMIOCM.DB2LUW_MONTABLESPACE_HIS where IBMIOCM_DATABASE='{DB_CONN_ID}' and tbsp_name = '{TBSP_NAME}' and IBMIOCM_TIMESTAMP >= '{START_TIME}' and IBMIOCM_TIMESTAMP < '{END_TIME}' order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)\n",
    "    if os.path.exists(\"tablespace.csv\"):\n",
    "        !rm tablespace.csv\n",
    "    if df_interval.empty:\n",
    "        print 'For Tablespace Table: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save tablespace data for generating table\n",
    "        df_interval.to_csv(\"tablespace.csv\", index_label = \"INDEX\")\n",
    "        dataframe_tbsp = %sql select date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, tbsp_name, (tbsp_total_pages * tbsp_page_size) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, (tbsp_used_pages * tbsp_page_size) / 1024 / 1024 as TBSP_USED_SIZE_MB, (tbsp_free_pages * tbsp_page_size) / 1024 / 1024 as TBSP_FREE_SIZE_MB, (tbsp_usable_pages * tbsp_page_size) / 1024 /1024 as TBSP_USABLE_SIZE_MB, case when tbsp_usable_pages > 0 then decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT from IBMIOCM.DB2LUW_MONTABLESPACE_HIS where IBMIOCM_DATABASE='{DB_CONN_ID}' and tbsp_name = '{TBSP_NAME}' and IBMIOCM_TIMESTAMP < '{END_TIME}' order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)\n",
    "        x_hour = dataframe_tbsp['HOUR'].values\n",
    "        x_hour_list = list(x_hour)\n",
    "        y_tbsp_util_percent = dataframe_tbsp['TBSP_UTILIZATION_PERCENT'].values\n",
    "        y_tbsp_util_percent_list = list(y_tbsp_util_percent)\n",
    "        date_all = list(dataframe_tbsp['DATE'].values)\n",
    "        \n",
    "        ori_datetime_str = get_original_datatime_str(date_all, x_hour_list)\n",
    "        #To set predict_count according to INTERVAL and max value is 10\n",
    "        if(INTERVAL <= 10):\n",
    "            predict_count = float('%.1f' % INTERVAL)\n",
    "        else:\n",
    "            predict_count = 10.0\n",
    "\n",
    "        #To form datetime_str from date_ori for indexing\n",
    "        datetime_str = []\n",
    "        for id_1 in range(len(date_all)):\n",
    "            tmp_hour = str(x_hour_list[id_1])\n",
    "            if(len(tmp_hour) == 1):\n",
    "                tmp_hour = '0' + tmp_hour\n",
    "            tmp_dt = str(date_all[id_1]) + ' ' + tmp_hour\n",
    "            datetime_str.append(tmp_dt)\n",
    "            \n",
    "        '''\n",
    "        To complement data for no data some hours\n",
    "        '''\n",
    "        #To get start timestamp First \n",
    "        min_dt_str = datetime_str[0] + ':00:00'\n",
    "        min_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "        min_timestamp = time.mktime(min_tuple)\n",
    "            \n",
    "        #To get end timestamp \n",
    "        max_tuple = time.strptime(END_TIME[0:13] + \":00:00\", '%Y-%m-%d %H:00:00')\n",
    "        max_timestamp = time.mktime(max_tuple)\n",
    "\n",
    "        hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "        #clear date_all\n",
    "        date_all = []\n",
    "        #clear x_hour_list\n",
    "        x_hour_list = []\n",
    "        ori_dt_str = []\n",
    "        all_tbsp_util_list_1 = []\n",
    "\n",
    "        #Reassign 4 variables above\n",
    "        for id_2 in range(hour_diff):\n",
    "            tmp_st = min_timestamp + id_2 * 3600.0\n",
    "            #To change timestamp to datetime string\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "            tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "            tmp_date_str = tmp_datetime_str[0:10]\n",
    "            tmp_hour_str = tmp_datetime_str[11:13]\n",
    "            ori_dt_str.append(tmp_date_str + ' ' + tmp_hour_str)\n",
    "            date_all.append(tmp_date_str)\n",
    "            x_hour_list.append(tmp_hour_str)\n",
    "\n",
    "            '''\n",
    "            Below code is for handling y-axis's data\n",
    "            If there is no data at this hour,0.0 will be filled into\n",
    "            '''\n",
    "            if tmp_datetime_str in ori_datetime_str:\n",
    "                tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                all_tbsp_util_list_1.append(y_tbsp_util_percent_list[tmp_index])\n",
    "            else:\n",
    "                all_tbsp_util_list_1.append(1.0)\n",
    "            \n",
    "        df_final = pd.Series(all_tbsp_util_list_1)\n",
    "        df_final.index = pd.PeriodIndex(start = ori_dt_str[0], end = ori_dt_str[len(ori_dt_str) - 1])\n",
    "        tbsp_model = sm.tsa.ARIMA(df_final,(7,0,1)).fit()\n",
    "            \n",
    "        #To get end_timestamp from ori_dt_str for preparing to form x_ticks and x_ticks_lables\n",
    "        end_dt_str = ori_dt_str[len(ori_dt_str) - 1] + \":00:00\"\n",
    "        end_tuple = time.strptime(end_dt_str, '%Y-%m-%d %H:00:00')\n",
    "        end_timestamp = time.mktime(end_tuple)\n",
    "            \n",
    "        #The count of output original data \n",
    "        ori_hour = None\n",
    "        ori_hour_st = end_timestamp - 3600.0 * (INTERVAL - 1)\n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(ori_hour_st)\n",
    "        ori_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "\n",
    "        #The count of predict data \n",
    "        predict_hour = None\n",
    "        predict_hour_st = None\n",
    "        #If INTERVAL>10，we will predict only 10 hours\n",
    "        #If not we we will predict INTERVAL hours\n",
    "        if(INTERVAL <= 10):\n",
    "            predict_hour_st = end_timestamp + 3600.0 * INTERVAL\n",
    "        else:\n",
    "            predict_hour_st = end_timestamp + 3600 * predict_count\n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(predict_hour_st)\n",
    "        predict_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "            \n",
    "        #To get predict data\n",
    "        predict_array = tbsp_model.predict(ori_dt_str[len(ori_dt_str) - 1], predict_hour, dynamic=True)\n",
    "        predict_array_list = list(predict_array)\n",
    "        predict_array_list[0] = all_tbsp_util_list_1[len(predict_array_list) - 1]\n",
    "        predict_array = np.asarray(predict_array_list[1:len(predict_array_list)])\n",
    "        all_tbsp_util_list_1 = all_tbsp_util_list_1[-INTERVAL:] + list(predict_array)\n",
    "           \n",
    "        #To set x ticks and x label\n",
    "        loop_count = int((predict_hour_st - ori_hour_st)/3600.0)\n",
    "        date_all = []\n",
    "        x_hour = []\n",
    "        for id_3 in range(loop_count + 1):\n",
    "            tmp_hour_st = ori_hour_st + id_3 * 3600.0\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(tmp_hour_st)\n",
    "            date_all.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:10])\n",
    "            x_hour.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[11:13])\n",
    "\n",
    "        x_ticks = []\n",
    "        x_ticks_lables = []\n",
    "        if len(x_ticks) == 0:\n",
    "            format_x_axis(date_all, x_hour, x_ticks, x_ticks_lables)\n",
    "\n",
    "        #To declare a Sketchpad\n",
    "        fig = pl.figure()\n",
    "        #To declare an ax container as a drawing paper\n",
    "        ax_tbsp_util= fig.add_subplot(111)\n",
    "\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        #To set the title/label/grid for the graph\n",
    "        figure_title = 'The Table Space Utilization Percent by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        x_lable = 'Hours'\n",
    "        #To set x-axis label\n",
    "        pl.xlabel(x_lable)\n",
    "        #To set y-axis label\n",
    "        pl.ylabel(u'TBSP_UTILIZATION_PERCENT %')\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        \n",
    "        #To scatter the data of y-axis and mark the data point\n",
    "        for id_5 in range(len(all_tbsp_util_list_1)):\n",
    "                if(all_tbsp_util_list_1[id_5] == 0.0):#If no data,drawing a empty circle\n",
    "                    if id_5 >= len(all_tbsp_util_list_1) - predict_count and id_5 < len(all_tbsp_util_list_1):\n",
    "                        #This is the data of prediction and empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = 'r') \n",
    "                    else:\n",
    "                        #This is the real data of y_max_log_percent and empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:#Data not empty\n",
    "                    if id_5 >= len(all_tbsp_util_list_1) - predict_count and id_5 < len(all_tbsp_util_list_1):\n",
    "                        #This is the data of prediction and not empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = 'r') \n",
    "                        pl.text(x_ticks[id_5], all_tbsp_util_list_1[id_5], '%.0f' % all_tbsp_util_list_1[id_5], fontsize = 9)\n",
    "                    else:\n",
    "                        #This is the real data of y_max_log_percent and not empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = '#4c78fb') \n",
    "                        pl.text(x_ticks[id_5], all_tbsp_util_list_1[id_5], '%.0f' % all_tbsp_util_list_1[id_5], fontsize = 9)\n",
    "\n",
    "        #Below two variable is used for storeing magnified data\n",
    "        xnew_hour = []\n",
    "        ynew_tbsp_util_percent = []\n",
    "        #Expand each x axis data 20 times\n",
    "        xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "        #Handle the data of new y axis data\n",
    "        ynew_tbsp_util_percent = spline(np.asarray(x_ticks), np.asarray(all_tbsp_util_list_1), xnew_hour)\n",
    "        ynew_tbsp_util_percent_list = list(ynew_tbsp_util_percent)\n",
    "        #No negative value for y-axis\n",
    "        for y_idx in range(len(ynew_tbsp_util_percent_list)):\n",
    "            if (ynew_tbsp_util_percent_list[y_idx] < 0.0):\n",
    "                ynew_tbsp_util_percent_list[y_idx] = 0.0\n",
    "        ynew_tbsp_util_percent = np.asarray(ynew_tbsp_util_percent_list)\n",
    "\n",
    "        #Fill the gragh according to your requirement\n",
    "        tbsp_ratio = len(list(predict_array)) / float('%.1f' % len(all_tbsp_util_list_1))\n",
    "        #pl.fill_between(xnew_hour, ynew_tbsp_util_percent, where=(xnew_hour.min() < xnew_hour) & (xnew_hour < xnew_hour.max() * (1.0 - tbsp_ratio)), color = '#4c78fb', alpha = 0.15)\n",
    "        pl.fill_between(xnew_hour, ynew_tbsp_util_percent, where=(xnew_hour.max() * (1.0 - tbsp_ratio) < xnew_hour) & (xnew_hour < xnew_hour.max()), color = '#2c628b', alpha = 0.1)\n",
    "        #Draw curve graph\n",
    "        pl.plot(xnew_hour, ynew_tbsp_util_percent, color = '#2c628b')\n",
    "        pl.yticks(np.arange(0, 101, 10))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import warnings\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.interpolate import spline\n",
    "from sklearn import linear_model\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "\n",
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'DATABASE')):\n",
    "    #To store all table spaces' usage information for drawing graph\n",
    "    all_db_util_list = []\n",
    "    #x ticks sorted by asc\n",
    "    x_ticks = None\n",
    "    #real x ticks label\n",
    "    x_ticks_labels = None\n",
    "    #per legend related with each table space\n",
    "    legend_title_db = []\n",
    "    #To get data from the target table to assemble a data frame that will be used for table view\n",
    "    #df_interval = %sql with temp as (select ibmiocm_database as dbname, IBMIOCM_TIMESTAMP, decimal(round(decimal((0.000000954*SUM(TBSP_USED_PAGES * TBSP_PAGE_SIZE)),17,2),2), 17,2) AS USAGE_GB FROM IBMIOCM.DATABASE_HIS where IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}'  group by ibmiocm_database, IBMIOCM_TIMESTAMP ORDER BY ibmiocm_database, IBMIOCM_TIMESTAMP), temp_stg as(select dbname as dbname, date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, max(USAGE_GB) as MAX_USAGE_GB from temp group by dbname, date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP) ORDER BY dbname, date, hour), temp_stg2 as(select row_number() over (partition by date order by MAX_USAGE_GB desc) as row_id, dbname, date, hour, MAX_USAGE_GB from temp_stg) select dbname, date, hour, MAX_USAGE_GB, row_id as rank from temp_stg2 where row_id <= 5\n",
    "    df_interval = %sql with temp as (select ibmiocm_database as dbname, IBMIOCM_TIMESTAMP, decimal(round(decimal((0.000000954*SUM(TBSP_USED_PAGES * TBSP_PAGE_SIZE)),17,2),2), 17,2) AS USAGE_GB FROM IBMIOCM.DATABASE_HIS where IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}'  group by ibmiocm_database, IBMIOCM_TIMESTAMP ORDER BY ibmiocm_database, IBMIOCM_TIMESTAMP), temp_stg as(select dbname as dbname, date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, max(USAGE_GB) as MAX_USAGE_GB from temp group by dbname, date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP) ORDER BY dbname, date, hour), temp_stg2 as(select row_number() over (partition by date,hour order by MAX_USAGE_GB desc) as row_id, dbname, date, hour, MAX_USAGE_GB from temp_stg) select dbname, date, hour, MAX_USAGE_GB, row_id as rank from temp_stg2 where row_id <= 5\n",
    "    if os.path.exists(\"db.csv\"):\n",
    "        !rm db.csv\n",
    "    if df_interval.empty:\n",
    "        print 'For databases: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To get top5 database\n",
    "        db_name_list = list(df_interval['DBNAME'].values)\n",
    "        db_name_set = set(db_name_list)\n",
    "        db_name_list = list(db_name_set)\n",
    "        #To judge whether file exists or not and if yes, delete it, if not save the data into a file db.csv\n",
    "        df_interval.to_csv(\"db.csv\", index_label = \"INDEX\")\n",
    "        for id_0 in range(len(db_name_list)):\n",
    "            tmp_db_name_str = str(db_name_list[id_0])\n",
    "            legend_title_db.append(tmp_db_name_str)\n",
    "            df_db_hour = %sql with temp as (select ibmiocm_database as dbname, IBMIOCM_TIMESTAMP, decimal(round(decimal((0.000000954*SUM(TBSP_USED_PAGES * TBSP_PAGE_SIZE)),17,2),2), 17,2) AS USAGE_GB FROM IBMIOCM.DATABASE_HIS where ibmiocm_database = '{tmp_db_name_str}' and IBMIOCM_TIMESTAMP < '{END_TIME}' group by ibmiocm_database, IBMIOCM_TIMESTAMP ORDER BY ibmiocm_database, IBMIOCM_TIMESTAMP), temp_stg as(select dbname as dbname, date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, max(USAGE_GB) as MAX_USAGE_GB from temp group by dbname, date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP) ORDER BY dbname, date, hour), temp_stg2 as(select row_number() over (partition by date order by MAX_USAGE_GB desc) as row_id, dbname, date, hour, MAX_USAGE_GB from temp_stg) select dbname, date, hour, MAX_USAGE_GB, row_id as rank from temp_stg2\n",
    "            #To get the hour data as x-axis from the data frame and convert the data into a numpy array\n",
    "            x_hour = df_db_hour['HOUR'].values\n",
    "            x_hour_list = list(x_hour)\n",
    "            #To get the data as y-axis from the data frame and convert the data into a numpy array\n",
    "            y_db_util_percent = df_db_hour['MAX_USAGE_GB'].values\n",
    "            #To chenage y_tbsp_util_percent into y_db_util_percent_list\n",
    "            y_db_util_percent_list = list(y_db_util_percent)\n",
    "            \n",
    "            date_all = list(df_db_hour['DATE'].values)\n",
    "            \n",
    "            ori_datetime_str = get_original_datatime_str(date_all, x_hour_list)\n",
    "            \n",
    "            #To set predict_count according to INTERVAL and max value is 10\n",
    "            if(INTERVAL <= 10):\n",
    "                predict_count = float('%.1f' % INTERVAL)\n",
    "            else:\n",
    "                predict_count = 10.0\n",
    "            \n",
    "            #To form datetime_str from date_ori for indexing\n",
    "            datetime_str = []\n",
    "            for id_1 in range(len(date_all)):\n",
    "                tmp_hour = str(x_hour_list[id_1])\n",
    "                if(len(tmp_hour) == 1):\n",
    "                    tmp_hour = '0' + tmp_hour\n",
    "                tmp_dt = str(date_all[id_1]) + ' ' + tmp_hour\n",
    "                datetime_str.append(tmp_dt)\n",
    "            \n",
    "            '''\n",
    "            To complement data for no data some hours\n",
    "            '''\n",
    "            #To get start timestamp First \n",
    "            min_dt_str = datetime_str[0] + ':00:00'\n",
    "            min_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "            min_timestamp = time.mktime(min_tuple)\n",
    "            \n",
    "            #To get end timestamp \n",
    "            max_tuple = time.strptime(END_TIME[0:13] + \":00:00\", '%Y-%m-%d %H:00:00')\n",
    "            max_timestamp = time.mktime(max_tuple)\n",
    "           \n",
    "            hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "            all_db_util_list_1 = []\n",
    "            #clear date_all\n",
    "            date_all = []\n",
    "            #clear x_hour_list\n",
    "            x_hour_list = []\n",
    "            ori_dt_str = []\n",
    "            #Reassign three variables above\n",
    "            for id_2 in range(hour_diff):\n",
    "                tmp_st = min_timestamp + id_2 * 3600.0\n",
    "                #To change timestamp to datetime string\n",
    "                tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "                tmp_date_str = tmp_datetime_str[0:10]\n",
    "                tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                ori_dt_str.append(tmp_date_str + ' ' + tmp_hour_str)\n",
    "                date_all.append(tmp_date_str)\n",
    "                x_hour_list.append(tmp_hour_str)\n",
    "\n",
    "                '''\n",
    "                Below code is for handling y-axis's data\n",
    "                If there is no data at this hour,0.0 will be filled into\n",
    "                '''\n",
    "                if tmp_datetime_str in ori_datetime_str:\n",
    "                    tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                    all_db_util_list_1.append(y_db_util_percent_list[tmp_index])\n",
    "                else:\n",
    "                    all_db_util_list_1.append(1.0)\n",
    "            #all_db_util_list.append(all_db_util_list_1)       \n",
    "            df_final = pd.Series(all_db_util_list_1)\n",
    "            df_final.index = pd.PeriodIndex(start = ori_dt_str[0], end = ori_dt_str[len(ori_dt_str) - 1])\n",
    "            db_model = sm.tsa.ARIMA(df_final, (7,0,1)).fit()\n",
    "            \n",
    "            #To get end_timestamp from ori_dt_str for preparing to form x_ticks and x_ticks_lables\n",
    "            end_dt_str = ori_dt_str[len(ori_dt_str) - 1] + \":00:00\"\n",
    "            end_tuple = time.strptime(end_dt_str, '%Y-%m-%d %H:00:00')\n",
    "            end_timestamp = time.mktime(end_tuple)\n",
    "            \n",
    "            #The count of output original data \n",
    "            ori_hour = None\n",
    "            ori_hour_st = end_timestamp - 3600.0 * (INTERVAL - 1)\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(ori_hour_st)\n",
    "            ori_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "\n",
    "            #The count of predict data \n",
    "            predict_hour = None\n",
    "            predict_hour_st = None\n",
    "            #If INTERVAL>10，we will predict only 10 hours\n",
    "            #If not we we will predict INTERVAL hours\n",
    "            if(INTERVAL <= 10):\n",
    "                predict_hour_st = end_timestamp + 3600.0 * INTERVAL\n",
    "            else:\n",
    "                predict_hour_st = end_timestamp + 3600 * predict_count\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(predict_hour_st)\n",
    "            predict_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "            \n",
    "            #To get predict data\n",
    "            predict_array = db_model.predict(ori_dt_str[len(ori_dt_str) - 1], predict_hour, dynamic=True)\n",
    "            predict_array_list = list(predict_array)\n",
    "            #If predict_array including NaN value,change them into 0.0\n",
    "            for i in range(len(predict_array_list)):\n",
    "                if np.isnan(float(predict_array_list[i])):\n",
    "                    predict_array_list[i] = 0.0\n",
    "            #predict_array = np.asarray(predict_array_list[1:len(predict_array_list)])\n",
    "            all_db_util_list_1 = all_db_util_list_1[-INTERVAL:] + predict_array_list[1:len(predict_array_list)]\n",
    "            all_db_util_list.append(all_db_util_list_1)\n",
    "            \n",
    "            #To set x ticks and x label\n",
    "            loop_count = int((predict_hour_st - ori_hour_st)/3600.0)\n",
    "            date_all = []\n",
    "            x_hour = []\n",
    "            for id_3 in range(loop_count + 1):\n",
    "                tmp_hour_st = ori_hour_st + id_3 * 3600.0\n",
    "                tmp_datetime = datetime.datetime.fromtimestamp(tmp_hour_st)\n",
    "                date_all.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:10])\n",
    "                x_hour.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[11:13])\n",
    "            x_ticks = []\n",
    "            x_ticks_lables = []\n",
    "            if len(x_ticks) == 0:\n",
    "                format_x_axis(date_all, x_hour, x_ticks, x_ticks_lables)\n",
    "        \n",
    "        if len(all_db_util_list) > 0:\n",
    "            #To declare color set\n",
    "            color_set = ['#ffb90f', '#a6266e', '#4c78fb', '#2c628b', '#60bdae']\n",
    "            #To declare a Sketchpad\n",
    "            fig = pl.figure()\n",
    "            #To declare an ax container as a drawing paper\n",
    "            ax_db_util= fig.add_subplot(111)\n",
    "\n",
    "            data_size = len(x_ticks)\n",
    "            if(data_size <= 20):\n",
    "                fig.set_size_inches(12,6)\n",
    "            elif(data_size <= 40):\n",
    "                fig.set_size_inches(16,6)\n",
    "            elif(data_size <= 60):\n",
    "                fig.set_size_inches(18,7)\n",
    "            elif(data_size <=100):\n",
    "                fig.set_size_inches(22,7)\n",
    "\n",
    "            #To set the title/label/grid for the graph\n",
    "            figure_title = 'The Max Database Usage by Hour\\n'\n",
    "            pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "            x_lable = 'Hours'\n",
    "            #To set x-axis label\n",
    "            pl.xlabel(x_lable)\n",
    "            #To set y-axis label\n",
    "            pl.ylabel(u'DATABASE MAX_USAGE_GB')\n",
    "            #To set grid line style according to your requirement\n",
    "            pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "            pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        \n",
    "            #Below two variable is used for storeing magnified data\n",
    "            xnew_hour = []\n",
    "            ynew_db_util_percent = []\n",
    "        \n",
    "            for id_4 in range(len(all_db_util_list)):\n",
    "                #To scatter the data of y-axis and mark the data point\n",
    "                for id_5 in range(len(all_db_util_list[id_4])):\n",
    "                    if(all_db_util_list[id_4][id_5] == 0.0):#If no data,drawing a empty circle\n",
    "                        if id_5 >= len(all_db_util_list[id_4]) - predict_count and id_5 < len(all_db_util_list[id_4]):\n",
    "                            #This is the data of prediction and empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = 'r') \n",
    "                        else:\n",
    "                            #This is the real data of y_max_log_percent and empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                    else:#Data not empty\n",
    "                        if id_5 >= len(all_db_util_list[id_4]) - predict_count and id_5 < len(all_db_util_list[id_4]):\n",
    "                            #This is the data of prediction and not empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = 'r') \n",
    "                            pl.text(x_ticks[id_5], all_db_util_list[id_4][id_5], '%.0f' % all_db_util_list[id_4][id_5], fontsize = 9)\n",
    "                        else:\n",
    "                            #This is the real data of y_max_log_percent and not empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = '#4c78fb') \n",
    "                            pl.text(x_ticks[id_5], all_db_util_list[id_4][id_5], '%.0f' % all_db_util_list[id_4][id_5], fontsize = 9)\n",
    "\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_db_util_percent = spline(np.asarray(x_ticks), np.asarray(all_db_util_list[id_4]), xnew_hour)\n",
    "                ynew_db_util_percent_list = list(ynew_db_util_percent)\n",
    "                #No negative value for y-axis\n",
    "                for y_idx in range(len(ynew_db_util_percent_list)):\n",
    "                    if (ynew_db_util_percent_list[y_idx] < 0.0):\n",
    "                        ynew_db_util_percent_list[y_idx] = 0.0\n",
    "                ynew_db_util_percent = np.asarray(ynew_db_util_percent_list)\n",
    "                #Fill the gragh according to your requirement\n",
    "                db_ratio = len(list(predict_array)) / float('%.1f' % len(all_db_util_list[id_4]))\n",
    "                #pl.fill_between(xnew_hour, ynew_db_util_percent, where=(xnew_hour.min() < xnew_hour) & (xnew_hour < xnew_hour.max() * (1.0 - tbsp_ratio)), color = '#4c78fb', alpha = 0.15)\n",
    "                pl.fill_between(xnew_hour, ynew_db_util_percent, where=(xnew_hour.max() * (1.0 - db_ratio) < xnew_hour) & (xnew_hour < xnew_hour.max()), color = '#2c628b', alpha = 0.1)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_db_util_percent, color = color_set[id_4])\n",
    "            #Set the legends for the both graphs\n",
    "            box_db = ax_db_util.get_position()\n",
    "            ax_db_util.set_position([box_db.x0, box_db.y0 + box_db.height * 0.1, box_db.width, box_db.height * 0.9])\n",
    "            ax_db_util.legend(legend_title_db, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_db))\n",
    "            #pl.yticks(np.arange(0, 101, 10))\n",
    "            pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
