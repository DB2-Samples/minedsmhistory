{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the required lib\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "'''\n",
    "To get query condition string from reading the file :singleDBCondition\n",
    "and handle it to gain all parameters by split function\n",
    "'''\n",
    "DB_CONN_ID = ''\n",
    "END_TIME = ''\n",
    "INTERVAL = 0\n",
    "START_TIME = ''\n",
    "REPORT_TYPE = ''\n",
    "ISOK_ALL_PARA = 1\n",
    "DBs = []\n",
    "GENERAL_ERROR = None\n",
    "\n",
    "def handle_query_condition():\n",
    "    # To declare below variables are global variables\n",
    "    global DB_CONN_ID\n",
    "    global END_TIME\n",
    "    global INTERVAL\n",
    "    global START_TIME\n",
    "    global REPORT_TYPE\n",
    "    global ISOK_ALL_PARA\n",
    "    global DBs\n",
    "    global GENERAL_ERROR\n",
    "    \n",
    "    # To read parameters from singleDBCondition file\n",
    "    where_cause = !cat enterpriseCondition\n",
    "    where_cause = re.search(r'DB_CONN_ID\\s?=\\s?.+END_TIME\\s?=\\s?.+INTERVAL\\s?=\\s?.+REPORT_TYPE\\s?=\\s?.+', where_cause[0])\n",
    "    GENERAL_ERROR = []\n",
    "    if (where_cause <> None):\n",
    "        all_vars = where_cause.group().split(' ')\n",
    "        if len(all_vars) <> 4:\n",
    "            err_str = 'The format of parameters is error.'\n",
    "            if err_str not in GENERAL_ERROR:\n",
    "                GENERAL_ERROR.append(err_str)\n",
    "            ISOK_ALL_PARA = 0\n",
    "        else:# len(all_vars) == 4\n",
    "            # To get DB_CONN_ID\n",
    "            if (re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]) == None):\n",
    "                err_str = 'The DB_CONN_ID can not be empty.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                DB_CONN_ID = re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]).group(1)\n",
    "                if (DB_CONN_ID <> '*'):\n",
    "                    DBs = DB_CONN_ID.split(',')\n",
    "                else:# * stands for getting all connection name from repository db\n",
    "                    dataframe_dbs = %sql select distinct substr(dbconn_id,1,30) as conn_name from ibm_dsm_views.throughput_all\n",
    "                    dbs = list(dataframe_dbs['CONN_NAME'].values)\n",
    "                    DBs = []\n",
    "                    for id_db in range(len(dbs)):\n",
    "                        DBs.append(str(dbs[id_db]).strip())\n",
    "\n",
    "            # To get the value of INTERVAL\n",
    "            if (re.search(r'INTERVAL\\s?=\\s?([0-9]+$)', all_vars[2]) == None):\n",
    "                err_str = 'The format of INTERVAL error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                INTERVAL = int(re.search(r'(\\d+)', all_vars[2]).group())\n",
    "                if (INTERVAL > 100):  # most get 100 data\n",
    "                    INTERVAL = 100\n",
    "\n",
    "            # To get START_TIME according to END_TIME\n",
    "            if (re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]) == None) | ( re.search(r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]) == None):\n",
    "                err_str = 'The format of END_TIME error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                \n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                END_TIME = re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]).group() + ' ' + re.search( r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]).group()\n",
    "                def is_valid_datetime(END_TIME):\n",
    "                    try:\n",
    "                        time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                        return True\n",
    "                    except:\n",
    "                        return False\n",
    "                \n",
    "                if is_valid_datetime(END_TIME) == True:\n",
    "                    datetime_tuple = time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                    # To slice datetime_tuple to gain exact time data\n",
    "                    year, month, day, hour, minute, second = datetime_tuple[:6]\n",
    "                    final_time = datetime.datetime(year, month, day, hour, minute, second) + datetime.timedelta(hours = -INTERVAL)\n",
    "                    # To transfer the datetime fields to string\n",
    "                    START_TIME = final_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                else:\n",
    "                    err_str = 'The format of END_TIME error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "            # To get the value of REPORT_TYPE\n",
    "            if (re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]) == None):\n",
    "                err_str = 'The format of REPORT_TYPE error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                REPORT_TYPE = re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]).group(1).upper()\n",
    "                if (REPORT_TYPE != 'ALL' and REPORT_TYPE != 'RESOURCE' and REPORT_TYPE != 'CPU' and REPORT_TYPE != 'MEMORY' and REPORT_TYPE != 'IO' and REPORT_TYPE != 'LOG'):\n",
    "                    err_str = 'The format of REPORT_TYPE error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "\n",
    "'''\n",
    "This function is to get all datatime string according to the result \n",
    "queried from database.\n",
    "\n",
    "The variable ori_datetime_str is a dictionary and its data \n",
    "from the combination of hour_list and date_all, which will \n",
    "be used for judging whether some data exists in it or not.\n",
    "'''\n",
    "def get_original_datatime_str(date_all, hour_list):\n",
    "    ori_datetime_str = {}\n",
    "    for indx in range(len(hour_list)):\n",
    "        tmp_date_str = date_all[indx].encode('unicode-escape').decode('string_escape')\n",
    "        tmp_hour_str = str(hour_list[indx])\n",
    "        tmp_datetime_str = ''\n",
    "        if(len(tmp_hour_str) == 1):#Change 1:00:00 into 01:00:00\n",
    "            tmp_hour_str = '0' + tmp_hour_str\n",
    "        tmp_datetime_str = tmp_date_str + ' ' + tmp_hour_str + ':00:00'\n",
    "        ori_datetime_str[tmp_datetime_str] = indx\n",
    "    return ori_datetime_str\n",
    "\n",
    "#To store previous date string value as a reference data\n",
    "def format_x_axis(date_all, hour_list, x_ticks, x_ticks_lables):\n",
    "    pre_date_str = str(date_all[0])\n",
    "\n",
    "    for dateIdx in range(len(date_all)):\n",
    "        # To get the data for the x-axis ticks\n",
    "        x_ticks.append(float('%0.1f' % dateIdx))\n",
    "        '''\n",
    "        Get the data for the lable of x-axis\n",
    "        If the label existed in the list x_ticks_lables,\n",
    "        put hour_str into x_ticks_lables \n",
    "        otherwise,put date_lables into x_ticks_lables\n",
    "        '''\n",
    "        hour_str = str(hour_list[dateIdx])\n",
    "        if len(hour_str) == 1:\n",
    "            hour_str = '0' + hour_str\n",
    "        date_str = str(date_all[dateIdx])\n",
    "        x_lables = date_str + ' ' + hour_str\n",
    "        if (dateIdx == 0):\n",
    "            x_ticks_lables.append(x_lables)\n",
    "        else:#dateIdx > 0\n",
    "            if (pre_date_str == date_str):\n",
    "                x_ticks_lables.append(hour_str)\n",
    "            else:\n",
    "                pre_date_str = date_str\n",
    "                x_ticks_lables.append(x_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "handle_query_condition()\n",
    "\n",
    "#To print the error of query conditon, only print once\n",
    "for err_id in range(len(GENERAL_ERROR)):\n",
    "    print GENERAL_ERROR[err_id]\n",
    "    \n",
    "resource_df_empty = False\n",
    "\n",
    "db_str = \"\"\n",
    "for id_1 in range(len(DBs)):\n",
    "    if id_1 == len(DBs) - 1:\n",
    "        db_str += \"'\" + DBs[id_1] + \"'\"\n",
    "    else:\n",
    "        db_str += \"'\" + DBs[id_1] + \"'\" + \",\"\n",
    "        \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #dataframe_week = %sql select year(collected) as year, week(collected) as week, substr(dbconn_id,1,30) as conn_name, date(min(collected)) as begin_date, date(max(collected)) as end_date, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(physical_reads_delta) as physical_reads, sum(act_completed_total_delta) as activities, sum(total_app_commits_delta) as commits, sum(total_act_time_delta) / (sum(act_aborted_total_delta) + sum(act_completed_total_delta)) as avg_activity_time_msec from ibm_dsm_views.throughput_all group by year(collected), week(collected), dbconn_id order by year(collected), week(collected), dbconn_id\n",
    "    dataframe_hour = %sql select char(date(collected)) as date, hour(collected) as hours, substr(dbconn_id,1,30) as conn_name, date(min(collected)) as begin_date, date(max(collected)) as end_date, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(physical_reads_delta) as physical_reads, sum(act_completed_total_delta) as activities, sum(total_app_commits_delta) as commits, sum(total_act_time_delta) / (sum(act_aborted_total_delta) + sum(act_completed_total_delta)) as avg_activity_time_msec from ibm_dsm_views.throughput_all where  dbconn_id in ({db_str}) and collected >= '{START_TIME}' and collected < '{END_TIME}' group by date(collected), hour(collected), dbconn_id order by date, hours, dbconn_id\n",
    "    if os.path.exists(\"resource.csv\"):\n",
    "        !rm resource.csv\n",
    "    if dataframe_hour.empty:\n",
    "        resource_df_empty = True\n",
    "        print 'For resource: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        dataframe_hour.to_csv(\"resource.csv\", index_label = \"INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "        \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    all_logical_read_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    y_logical_reads_max = 0\n",
    "    legend_title_logical = []\n",
    "    log_mil = False\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, hour(collected) as hours, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(PHYSICAL_READS_DELTA) as physical_reads, sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec,sum(act_completed_total_delta) as activities from ibm_dsm_views.throughput_all where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' group by date(collected), hour(collected) order by date, hours\n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(Logical Reads): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_logical.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_logical_reads = dataframe_hour['LOGICAL_READS']\n",
    "            y_logical_reads_list = list(y_logical_reads)\n",
    "            if (y_logical_reads_max < y_logical_reads.max()):\n",
    "                y_logical_reads_max = y_logical_reads.max()\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_logical_reads_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_logical_reads_list.append(list(y_logical_reads)[tmp_index])\n",
    "                    else:\n",
    "                        y_logical_reads_list.append(0)\n",
    "            all_logical_read_list.append(y_logical_reads_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_logical_read_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_log = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='Logical Reads by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        if y_logical_reads.max() > 1000000:\n",
    "            log_mil = True\n",
    "            for id_4 in range(len(all_logical_read_list)):\n",
    "                for id_4_0 in range(len(all_logical_read_list[id_4])):\n",
    "                    all_logical_read_list[id_4][id_4_0] = all_logical_read_list[id_4][id_4_0]/1000000.0\n",
    "        \n",
    "        xnew_hour = []\n",
    "        ynew_logical_reads = []\n",
    "        for id_5 in range(len(all_logical_read_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_logical_read_list[id_5])):\n",
    "                if(all_logical_read_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_logical_read_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_logical_read_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_logical_read_list[id_5][id_6], '%.1f' % all_logical_read_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_logical_read_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_logical_reads = spline(np.asarray(x_ticks), all_logical_read_list[id_5], xnew_hour)\n",
    "                ynew_logical_reads_list = list(ynew_logical_reads)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_logical_reads_list)):\n",
    "                    if (ynew_logical_reads_list[id_7] < 0.0):\n",
    "                        ynew_logical_reads_list[id_7] = 0.0\n",
    "                ynew_logical_reads = np.asarray(ynew_logical_reads_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_logical_reads, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_logical_reads)\n",
    "               \n",
    "        if (log_mil == True):\n",
    "            pl.ylabel(u'LOGICAL_READS(mil times)', fontsize = 12)\n",
    "        else:\n",
    "            pl.ylabel(u'LOGICAL_READS(times)',fontsize=12)\n",
    "        #pl.legend(labels = ['a', 'b'], loc = 'best')\n",
    "        #Set the legends for the both graphs\n",
    "        box_log = ax_log.get_position()\n",
    "        ax_log.set_position([box_log.x0, box_log.y0 + box_log.height * 0.1, box_log.width, box_log.height * 0.9])\n",
    "        ax_log.legend(legend_title_logical, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_logical))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "        \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #Declare a Sketchpad as the first graph\n",
    "    all_physical_read_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    y_physical_reads_max = 0\n",
    "    legend_title_physical = []\n",
    "    phy_mil = False\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, hour(collected) as hours, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(PHYSICAL_READS_DELTA) as physical_reads, sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec,sum(act_completed_total_delta) as activities from ibm_dsm_views.throughput_all where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' group by date(collected), hour(collected) order by date, hours\n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(Physical Reads): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_physical.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_physical_reads = dataframe_hour['PHYSICAL_READS']\n",
    "            y_physical_reads_list = list(y_physical_reads)\n",
    "            if (y_physical_reads_max < y_physical_reads.max()):\n",
    "                y_physical_reads_max = y_physical_reads.max()\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_physical_reads_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_physical_reads_list.append(list(y_physical_reads)[tmp_index])\n",
    "                    else:\n",
    "                        y_physical_reads_list.append(0)\n",
    "            all_physical_read_list.append(y_physical_reads_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_physical_read_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_phy = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='Physical Reads by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        if y_physical_reads.max() > 1000000:\n",
    "            phy_mil = True\n",
    "            for id_4 in range(len(all_physical_read_list)):\n",
    "                for id_4_0 in range(len(all_physical_read_list[id_4])):\n",
    "                    all_physical_read_list[id_4][id_4_0] = all_physical_read_list[id_4][id_4_0]/1000000.0\n",
    "        \n",
    "        xnew_hour = []\n",
    "        ynew_physical_reads = []\n",
    "        for id_5 in range(len(all_physical_read_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_physical_read_list[id_5])):\n",
    "                if(all_physical_read_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_physical_read_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_physical_read_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_physical_read_list[id_5][id_6], '%.1f' % all_physical_read_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_physical_read_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_physical_reads = spline(np.asarray(x_ticks), all_physical_read_list[id_5], xnew_hour)\n",
    "                ynew_physical_reads_list = list(ynew_physical_reads)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_physical_reads_list)):\n",
    "                    if (ynew_physical_reads_list[id_7] < 0.0):\n",
    "                        ynew_physical_reads_list[id_7] = 0.0\n",
    "                ynew_physical_reads = np.asarray(ynew_physical_reads_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_physical_reads, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_physical_reads)\n",
    "               \n",
    "        if (phy_mil == True):\n",
    "            pl.ylabel(u'PHYSICAL_READS(mil times)', fontsize = 12)\n",
    "        else:\n",
    "            pl.ylabel(u'PHYSICAL_READS(times)',fontsize=12)\n",
    "        #pl.legend(labels = ['a', 'b'], loc = 'best')\n",
    "        #Set the legends for the both graphs\n",
    "        box_phy = ax_phy.get_position()\n",
    "        ax_phy.set_position([box_phy.x0, box_phy.y0 + box_phy.height * 0.1, box_phy.width, box_phy.height * 0.9])\n",
    "        ax_phy.legend(legend_title_physical, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_physical))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "        \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #Declare a Sketchpad as the first graph\n",
    "    all_cpu_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    legend_title_cpu = []\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, hour(collected) as hours, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(PHYSICAL_READS_DELTA) as physical_reads, sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec,sum(act_completed_total_delta) as activities from ibm_dsm_views.throughput_all where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' group by date(collected), hour(collected) order by date, hours\n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(CPU_SEC): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_cpu.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_cpu = dataframe_hour['CPU_SEC']\n",
    "            y_cpu_list = list(y_cpu)\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_cpu_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_cpu_list.append(list(y_cpu)[tmp_index])\n",
    "                    else:\n",
    "                        y_cpu_list.append(0)\n",
    "            all_cpu_list.append(y_cpu_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_cpu_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_cpu = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='CPU Usage by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        xnew_hour = []\n",
    "        ynew_cpu = []\n",
    "        for id_5 in range(len(all_cpu_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_cpu_list[id_5])):\n",
    "                if(all_cpu_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_cpu_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_cpu_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_cpu_list[id_5][id_6], '%.1f' % all_cpu_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_cpu_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_cpu = spline(np.asarray(x_ticks), all_cpu_list[id_5], xnew_hour)\n",
    "                ynew_cpu_list = list(ynew_cpu)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_cpu_list)):\n",
    "                    if (ynew_cpu_list[id_7] < 0.0):\n",
    "                        ynew_cpu_list[id_7] = 0.0\n",
    "                ynew_cpu = np.asarray(ynew_cpu_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_cpu, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_cpu)\n",
    "               \n",
    "        pl.ylabel(u'CPU_SEC(s)',fontsize=12)\n",
    "        #Set the legends for the both graphs\n",
    "        box_cpu = ax_cpu.get_position()\n",
    "        ax_cpu.set_position([box_cpu.x0, box_cpu.y0 + box_cpu.height * 0.1, box_cpu.width, box_cpu.height * 0.9])\n",
    "        ax_cpu.legend(legend_title_cpu, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_cpu))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "        \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #Declare a Sketchpad as the first graph\n",
    "    all_avg_act_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    legend_title_avg_act = []\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, hour(collected) as hours, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(PHYSICAL_READS_DELTA) as physical_reads, sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec,sum(act_completed_total_delta) as activities from ibm_dsm_views.throughput_all where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' group by date(collected), hour(collected) order by date, hours\n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(AVG_ACTIVITY_TIME_MSEC): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_avg_act.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_avg_act = dataframe_hour['AVG_ACTIVITY_TIME_MSEC']\n",
    "            y_avg_act_list = list(y_avg_act)\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_avg_act_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_avg_act_list.append(list(y_avg_act)[tmp_index])\n",
    "                    else:\n",
    "                        y_avg_act_list.append(0)\n",
    "            all_avg_act_list.append(y_avg_act_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_avg_act_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_avg_act = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='Average Activity Time by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        xnew_hour = None\n",
    "        ynew_avg_act = None\n",
    "        for id_5 in range(len(all_avg_act_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_avg_act_list[id_5])):\n",
    "                if(all_avg_act_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_avg_act_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_avg_act_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_avg_act_list[id_5][id_6], '%.0f' % all_avg_act_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_avg_act_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_avg_act = spline(np.asarray(x_ticks), all_avg_act_list[id_5], xnew_hour)\n",
    "                ynew_avg_act_list = list(ynew_avg_act)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_avg_act_list)):\n",
    "                    if (ynew_avg_act_list[id_7] < 0.0):\n",
    "                        ynew_avg_act_list[id_7] = 0.0\n",
    "                ynew_avg_act = np.asarray(ynew_avg_act_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_avg_act, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_avg_act)\n",
    "               \n",
    "        pl.ylabel(u'AVG_ACTIVITY_TIME_MSEC', fontsize = 12)\n",
    "        #Set the legends for the both graphs\n",
    "        box_avg_act = ax_avg_act.get_position()\n",
    "        ax_avg_act.set_position([box_avg_act.x0, box_avg_act.y0 + box_avg_act.height * 0.1, box_avg_act.width, box_avg_act.height * 0.9])\n",
    "        ax_avg_act.legend(legend_title_avg_act, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_avg_act))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "handle_query_condition()\n",
    "\n",
    "#Query condition exists some error\n",
    "if len(GENERAL_ERROR) > 0 and os.path.exists(\"cpu.csv\"):\n",
    "    !rm cpu.csv\n",
    " \n",
    "def get_all_times(list_all, list_distinct):\n",
    "    try:\n",
    "        distinct_times = {}\n",
    "        for idx in range(len(list_distinct)):\n",
    "            cnt = 0\n",
    "            for idx_1 in range(len(list_all)):\n",
    "                if list_distinct[idx] == list_all[idx_1]:\n",
    "                    cnt += 1\n",
    "            distinct_times[str(list_distinct[idx])] = cnt\n",
    "        return distinct_times\n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "        \n",
    "def getmaxtime(conn_times):\n",
    "    try:\n",
    "        maxtime = 1\n",
    "        all_times = conn_times.values()\n",
    "        for idx in range(len(all_times)):\n",
    "            if maxtime < all_times[idx]:\n",
    "                maxtime = all_times[idx]\n",
    "        return maxtime\n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "        \n",
    "def get_sort_circle(mean_cpu_usage):\n",
    "    try:\n",
    "        circle_size = {}\n",
    "        dbname_keys = []\n",
    "        cpu_values = []\n",
    "        tmp_dict = {}\n",
    "        ##Get no duplicates values\n",
    "        for k,v in mean_cpu_usage.items():\n",
    "            dbname_keys.append(k)\n",
    "            cpu_values.append(v)\n",
    "        cpu_values = sorted(list(set(cpu_values)))\n",
    "        \n",
    "        ##Form a new dictionary to store above key and rank value\n",
    "        for i in range(len(cpu_values)):\n",
    "            tmp_dict[cpu_values[i]] = i + 1\n",
    "        \n",
    "        #Generate a new dictionary using new rank\n",
    "        for key in mean_cpu_usage:\n",
    "            for key1 in tmp_dict:\n",
    "                if mean_cpu_usage[key] == key1:\n",
    "                    circle_size[key] = tmp_dict[key1]\n",
    "        return circle_size\n",
    "    \n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "    \n",
    "def get_mean_rank(all_conn_names,distinct_conn_names, all_ranks):\n",
    "    try:\n",
    "        y_data = []\n",
    "        y_data_dict = {}\n",
    "        score_list = {1:13, 2:8, 3:5, 4:3, 5:2}\n",
    "        for i in range(len(distinct_conn_names)):\n",
    "            tmp_cnt = 0\n",
    "            tmp_score = 0\n",
    "            for j in range(len(all_conn_names)):\n",
    "                if str(distinct_conn_names[i]) == str(all_conn_names[j]):\n",
    "                    tmp_cnt += 1\n",
    "                    tmp_score += score_list[all_ranks[j]]\n",
    "            float_rank = float('%.1f' % (tmp_score * 1.0 / tmp_cnt))\n",
    "            y_data.append(float_rank)\n",
    "            y_data_dict[str(distinct_conn_names[i])] = float_rank\n",
    "        \n",
    "        float_sorted_rank = list(reversed(sorted(list(set(y_data)))))\n",
    "        int_rank_dict = {}\n",
    "        ##Form a new dictionary to store above key and rank value\n",
    "        for k in range(len(float_sorted_rank)):\n",
    "            int_rank_dict[float_sorted_rank[k]] = k + 1\n",
    "        \n",
    "        for f in range(len(y_data)):\n",
    "            for key in int_rank_dict:\n",
    "                if(y_data[f]) == key:\n",
    "                    y_data[f] = 6 - int_rank_dict[key]\n",
    "                    break\n",
    "        return y_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "    \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'CPU')):\n",
    "    dataframe_cpu = %sql with temp as(select row_number()over(partition by week(collected) order by sum(total_cpu_usec_delta)/1000000.0 desc) as row_id, month(collected) as month, week(collected) as week, dbconn_id as conn_name, decimal(sum(total_cpu_usec_delta)/1000000.0,17,2) as cpu_sec from ibm_dsm_views.throughput_all group by month(collected), week(collected), dbconn_id order by month, week, row_id) select month, week, conn_name, cpu_sec, row_id as rank from temp where row_id <= 5\n",
    "    if os.path.exists(\"cpu.csv\"):\n",
    "        !rm cpu.csv\n",
    "    if dataframe_cpu.empty:\n",
    "        print 'For CPU: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        dataframe_cpu.to_csv(\"cpu.csv\", index_label = \"INDEX\")\n",
    "        #For the x-labels\n",
    "        x_ticks_labels = list(set(list(dataframe_cpu['WEEK'].values)))\n",
    "        x_ticks = list(range(1, len(x_ticks_labels) + 1))\n",
    "        x_ticks_labels = list(set(list(dataframe_cpu['WEEK'].values)))\n",
    "        #For the y-labels\n",
    "        y_ticks = list(range(1, 6))\n",
    "        y_ticks_labels = list(reversed(y_ticks))\n",
    "        #Get all the connection names from the data frame\n",
    "        all_conn_names = list(dataframe_cpu['CONN_NAME'].values)\n",
    "        #Remove duplicates\n",
    "        distinct_conn_names = list(set(all_conn_names))\n",
    "        #For the size of scatter\n",
    "        conn_times = get_all_times(all_conn_names, distinct_conn_names)\n",
    "        #Get weeks\n",
    "        all_weeks = list(dataframe_cpu['WEEK'].values)\n",
    "        all_ranks = list(dataframe_cpu['RANK'].values)\n",
    "        \n",
    "        fig = pl.figure()\n",
    "        fig.set_size_inches(24,6)\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        #60 colors\n",
    "        all_colors = ['#988FD0', '#E04C32', '#368AB6', '#777777', '#FEBE67', '#66CDAA', '#0000CD', '#BA55D3', '#9370DB', '#3CB371', '#7B68EE', '#00FA9A', '#48D1CC', '#C71585', '#191970', '#F5FFFA', '#FFE4E1', '#FFE4B5', '#FFDEAD', '#000080', '#FDF5E6', '#808000', '#6B8E23', '#FFA500', '#FF4500', '#DA70D6', '#EEE8AA', '#98FB98', '#AFEEEE', '#DB7093', '#FFEFD5', '#FFDAB9', '#CD853F', '#FFC0CB', '#DDA0DD', '#B0E0E6', '#800080', '#FF0000', '#BC8F8F', '#4169E1', '#8B4513', '#FA8072', '#FAA460', '#2E8B57', '#FFF5EE', '#A0522D', '#C0C0C0', '#87CEEB', '#6A5ACD', '#708090', '#FFFAFA', '#00FF7F', '#4682B4', '#D2B48C', '#008080', '#D8BFD8', '#FF6347', '#40E0D0', '#EE82EE', '#F5DEB3']\n",
    "        ax1.set_title('Weekly Top 5 CPU Usage\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax1.set_xlabel('Weeks', fontsize = 12)\n",
    "        ax1.set_ylabel('Rank', fontsize = 12)  \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        ax1.set_xticklabels(x_ticks_labels)\n",
    "        ax1.set_yticks(y_ticks)\n",
    "        ax1.set_yticklabels(y_ticks_labels)\n",
    "        ax1.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        mean_cpu_usage = {}\n",
    "        #Get all the connection names from the data frame\n",
    "        all_cpu_usage = list(dataframe_cpu['CPU_SEC'].values)\n",
    "        \n",
    "        for id_0 in range(len(distinct_conn_names)):\n",
    "            x_data = []\n",
    "            y_data = []\n",
    "            cnt = 0\n",
    "            sum_cpu_usage = 0.0\n",
    "            for id_1 in range(len(all_conn_names)):\n",
    "                if(distinct_conn_names[id_0] == all_conn_names[id_1]):\n",
    "                    cnt += 1\n",
    "                    sum_cpu_usage += all_cpu_usage[id_1]\n",
    "                    x_data.append(x_ticks_labels.index(all_weeks[id_1]) + 1)\n",
    "                    y_data.append(6 - all_ranks[id_1])\n",
    "                \n",
    "            mean_cpu_usage[str(distinct_conn_names[id_0])] = float('%.1f' % (sum_cpu_usage / cnt))\n",
    "            ax1.scatter(x_data, y_data, c = all_colors[id_0], marker = 'o', edgecolors = 'r', s = 200 * conn_times[distinct_conn_names[id_0]])\n",
    "            ax1.text(x_data[0], y_data[0], '%s' % distinct_conn_names[id_0], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'right')\n",
    "            ax1.plot(x_data, y_data, linestyle = '--', color = all_colors[id_0], lw = 1.0)\n",
    "        \n",
    "        #Below is the second graph\n",
    "        ax2.set_title('Top CPU Usage Map\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax2.set_xlabel('Frequency', fontsize = 12)\n",
    "        ax2.set_ylabel('Rank', fontsize = 12)\n",
    "        max_times = getmaxtime(conn_times)\n",
    "        x_ticks = list(range(1, max_times + 1))\n",
    "        x_data = conn_times.values()\n",
    "        y_data = get_mean_rank(all_conn_names, distinct_conn_names, all_ranks)\n",
    "        circle_size = get_sort_circle(mean_cpu_usage)\n",
    "        ax2.set_xticks(x_ticks)\n",
    "        ax2.set_xticklabels(x_ticks)\n",
    "        ax2.set_yticks(y_ticks)\n",
    "        ax2.set_yticklabels(y_ticks_labels)\n",
    "        ax2.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        for ax2_idx in range(len(distinct_conn_names)):\n",
    "            ax2.scatter(x_data[ax2_idx], y_data[ax2_idx], c = all_colors[ax2_idx], marker = 'o', edgecolors = 'r', s = 200 * circle_size[distinct_conn_names[ax2_idx]])\n",
    "            ax2.text(x_data[ax2_idx], y_data[ax2_idx], '%s' % distinct_conn_names[ax2_idx], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'left')\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "handle_query_condition()\n",
    "\n",
    "#Query condition exists some error\n",
    "if len(GENERAL_ERROR) > 0 and os.path.exists(\"memory.csv\"):\n",
    "    !rm memory.csv\n",
    "\n",
    "def get_all_times(list_all, list_distinct):\n",
    "    try:\n",
    "        distinct_times = {}\n",
    "        for idx in range(len(list_distinct)):\n",
    "            cnt = 0\n",
    "            for idx_1 in range(len(list_all)):\n",
    "                if list_distinct[idx] == list_all[idx_1]:\n",
    "                    cnt += 1\n",
    "            distinct_times[str(list_distinct[idx])] = cnt\n",
    "        return distinct_times\n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "        \n",
    "def getmaxtime(conn_times):\n",
    "    try:\n",
    "        maxtime = 1\n",
    "        all_times = conn_times.values()\n",
    "        for idx in range(len(all_times)):\n",
    "            if maxtime < all_times[idx]:\n",
    "                maxtime = all_times[idx]\n",
    "        return maxtime\n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "        \n",
    "def get_sort_circle(mean_memory_usage):\n",
    "    try:\n",
    "        circle_size = {}\n",
    "        dbname_keys = []\n",
    "        memory_values = []\n",
    "        tmp_dict = {}\n",
    "        ##Get no duplicates values\n",
    "        for k,v in mean_memory_usage.items():\n",
    "            dbname_keys.append(k)\n",
    "            memory_values.append(v)\n",
    "        memory_values = sorted(list(set(memory_values)))\n",
    "        \n",
    "        ##Form a new dictionary to store above key and rank value\n",
    "        for i in range(len(memory_values)):\n",
    "            tmp_dict[memory_values[i]] = i + 1\n",
    "        \n",
    "        #Generate a new dictionary using new rank\n",
    "        for key in mean_memory_usage:\n",
    "            for key1 in tmp_dict:\n",
    "                if mean_memory_usage[key] == key1:\n",
    "                    circle_size[key] = tmp_dict[key1]\n",
    "        return circle_size\n",
    "    \n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "    \n",
    "def get_mean_rank(all_conn_names,distinct_conn_names, all_ranks):\n",
    "    try:\n",
    "        y_data = []\n",
    "        y_data_dict = {}\n",
    "        score_list = {1:13, 2:8, 3:5, 4:3, 5:2}\n",
    "        for i in range(len(distinct_conn_names)):\n",
    "            tmp_cnt = 0\n",
    "            tmp_score = 0\n",
    "            for j in range(len(all_conn_names)):\n",
    "                if str(distinct_conn_names[i]) == str(all_conn_names[j]):\n",
    "                    tmp_cnt += 1\n",
    "                    tmp_score += score_list[all_ranks[j]]\n",
    "            float_rank = float('%.1f' % (tmp_score * 1.0 / tmp_cnt))\n",
    "            y_data.append(float_rank)\n",
    "            y_data_dict[str(distinct_conn_names[i])] = float_rank\n",
    "        \n",
    "        float_sorted_rank = list(reversed(sorted(list(set(y_data)))))\n",
    "        int_rank_dict = {}\n",
    "        ##Form a new dictionary to store above key and rank value\n",
    "        for k in range(len(float_sorted_rank)):\n",
    "            int_rank_dict[float_sorted_rank[k]] = k + 1\n",
    "        \n",
    "        for f in range(len(y_data)):\n",
    "            for key in int_rank_dict:\n",
    "                if(y_data[f]) == key:\n",
    "                    y_data[f] = 6 - int_rank_dict[key]\n",
    "                    break\n",
    "        return y_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print str(e)\n",
    "    \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'MEMORY')):\n",
    "    dataframe_memory = %sql with temp as(select row_number()over(partition by week(collected) order by max(memory_pool_used_gb) desc) as row_id, month(collected) as month, week(collected) as week, dbconn_id as conn_name, dec(max(memory_pool_used_gb),17,2) as memory_usage_gb from IBM_DSM_VIEWS.MEM_DB_TOTAL_USED group by month(collected), week(collected), dbconn_id order by month, week, row_id) select month, week, conn_name, memory_usage_gb, row_id as rank from temp where row_id < 5\n",
    "    \n",
    "    if os.path.exists(\"memory.csv\"):\n",
    "        !rm memory.csv\n",
    "    if dataframe_memory.empty:\n",
    "        print 'For Memory: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        dataframe_memory.to_csv(\"memory.csv\", index_label = \"INDEX\")\n",
    "        \n",
    "        #For the x-labels\n",
    "        x_ticks_labels = list(set(list(dataframe_memory['WEEK'].values)))\n",
    "        x_ticks = list(range(1, len(x_ticks_labels) + 1))\n",
    "        x_ticks_labels = list(set(list(dataframe_memory['WEEK'].values)))\n",
    "        #For the y-labels\n",
    "        y_ticks = list(range(1, 6))\n",
    "        y_ticks_labels = list(reversed(y_ticks))\n",
    "        #Get all the connection names from the data frame\n",
    "        all_conn_names = list(dataframe_memory['CONN_NAME'].values)\n",
    "        #Remove duplicates\n",
    "        distinct_conn_names = list(set(all_conn_names))\n",
    "        #For the size of scatter\n",
    "        conn_times = get_all_times(all_conn_names, distinct_conn_names)\n",
    "        #Get weeks\n",
    "        all_weeks = list(dataframe_memory['WEEK'].values)\n",
    "        all_ranks = list(dataframe_memory['RANK'].values)\n",
    "        fig = pl.figure()\n",
    "        fig.set_size_inches(24,6)\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        #60 colors\n",
    "        all_colors = ['#988FD0', '#E04C32', '#368AB6', '#777777', '#FEBE67', '#66CDAA', '#0000CD', '#BA55D3', '#9370DB', '#3CB371', '#7B68EE', '#00FA9A', '#48D1CC', '#C71585', '#191970', '#F5FFFA', '#FFE4E1', '#FFE4B5', '#FFDEAD', '#000080', '#FDF5E6', '#808000', '#6B8E23', '#FFA500', '#FF4500', '#DA70D6', '#EEE8AA', '#98FB98', '#AFEEEE', '#DB7093', '#FFEFD5', '#FFDAB9', '#CD853F', '#FFC0CB', '#DDA0DD', '#B0E0E6', '#800080', '#FF0000', '#BC8F8F', '#4169E1', '#8B4513', '#FA8072', '#FAA460', '#2E8B57', '#FFF5EE', '#A0522D', '#C0C0C0', '#87CEEB', '#6A5ACD', '#708090', '#FFFAFA', '#00FF7F', '#4682B4', '#D2B48C', '#008080', '#D8BFD8', '#FF6347', '#40E0D0', '#EE82EE', '#F5DEB3']\n",
    "        ax1.set_title('Weekly Top 5 Memory Usage\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax1.set_xlabel('Weeks', fontsize = 12)\n",
    "        ax1.set_ylabel('Rank', fontsize = 12)  \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        ax1.set_xticklabels(x_ticks_labels)\n",
    "        ax1.set_yticks(y_ticks)\n",
    "        ax1.set_yticklabels(y_ticks_labels)\n",
    "        ax1.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        mean_memory_usage = {}\n",
    "        #Get all the connection names from the data frame\n",
    "        all_memory_usage = list(dataframe_memory['MEMORY_USAGE_GB'].values)\n",
    "\n",
    "        for id_0 in range(len(distinct_conn_names)):\n",
    "            x_data = []\n",
    "            y_data = []\n",
    "            cnt = 0\n",
    "            sum_memory_usage = 0.0\n",
    "            for id_1 in range(len(all_conn_names)):\n",
    "                if(distinct_conn_names[id_0] == all_conn_names[id_1]):\n",
    "                    cnt += 1\n",
    "                    sum_memory_usage += all_memory_usage[id_1]\n",
    "                    x_data.append(x_ticks_labels.index(all_weeks[id_1]) + 1)\n",
    "                    y_data.append(6 - all_ranks[id_1])\n",
    "            mean_memory_usage[str(distinct_conn_names[id_0])] = float('%.1f' % (sum_memory_usage / cnt))\n",
    "            ax1.scatter(x_data, y_data, c = all_colors[id_0], marker = 'o', edgecolors = 'r', s = 200 * conn_times[distinct_conn_names[id_0]])\n",
    "            ax1.text(x_data[0], y_data[0], '%s' % distinct_conn_names[id_0], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'right')\n",
    "            ax1.plot(x_data, y_data, linestyle = '--', color = all_colors[id_0], lw = 1.0)\n",
    "        \n",
    "        #Below is the second graph\n",
    "        ax2.set_title('Top Memory Usage Map\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax2.set_xlabel('Frequency', fontsize = 12)\n",
    "        ax2.set_ylabel('Rank', fontsize = 12)\n",
    "        max_times = getmaxtime(conn_times)\n",
    "        x_ticks = list(range(1, max_times + 1))\n",
    "        x_data = conn_times.values()\n",
    "        y_data = get_mean_rank(all_conn_names, distinct_conn_names, all_ranks)\n",
    "        circle_size = get_sort_circle(mean_memory_usage)\n",
    "        ax2.set_xticks(x_ticks)\n",
    "        ax2.set_xticklabels(x_ticks)\n",
    "        ax2.set_yticks(y_ticks)\n",
    "        ax2.set_yticklabels(y_ticks_labels)\n",
    "        ax2.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        for ax2_idx in range(len(distinct_conn_names)):\n",
    "            ax2.scatter(x_data[ax2_idx], y_data[ax2_idx], c = all_colors[ax2_idx], marker = 'o', edgecolors = 'r', s = 200 * circle_size[distinct_conn_names[ax2_idx]])\n",
    "            ax2.text(x_data[ax2_idx], y_data[ax2_idx], '%s' % distinct_conn_names[ax2_idx], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'left')\n",
    "        pl.show()       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
