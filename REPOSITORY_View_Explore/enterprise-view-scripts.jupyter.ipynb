{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the required lib\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "'''\n",
    "To get query condition string from reading the file :singleDBCondition\n",
    "and handle it to gain all parameters by split function\n",
    "'''\n",
    "DB_CONN_ID = ''\n",
    "END_TIME = ''\n",
    "INTERVAL = 0\n",
    "START_TIME = ''\n",
    "REPORT_TYPE = ''\n",
    "ISOK_ALL_PARA = 1\n",
    "DBs = []\n",
    "GENERAL_ERROR = None\n",
    "\n",
    "def handle_query_condition():\n",
    "    # To declare below variables are global variables\n",
    "    global DB_CONN_ID\n",
    "    global END_TIME\n",
    "    global INTERVAL\n",
    "    global START_TIME\n",
    "    global REPORT_TYPE\n",
    "    global ISOK_ALL_PARA\n",
    "    global DBs\n",
    "    global GENERAL_ERROR\n",
    "    \n",
    "    # To read parameters from singleDBCondition file\n",
    "    where_cause = !cat queryCondition.txt\n",
    "    where_cause = re.search(r'DB_CONN_ID\\s?=\\s?.+END_TIME\\s?=\\s?.+INTERVAL\\s?=\\s?.+REPORT_TYPE\\s?=\\s?.+', where_cause[0])\n",
    "    GENERAL_ERROR = []\n",
    "    if (where_cause <> None):\n",
    "        all_vars = where_cause.group().split(' ')\n",
    "        if len(all_vars) <> 4:\n",
    "            err_str = 'The format of parameters is error.'\n",
    "            if err_str not in GENERAL_ERROR:\n",
    "                GENERAL_ERROR.append(err_str)\n",
    "            ISOK_ALL_PARA = 0\n",
    "        else:# len(all_vars) == 4\n",
    "            # To get DB_CONN_ID\n",
    "            if (re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]) == None):\n",
    "                err_str = 'The DB_CONN_ID can not be empty.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                DB_CONN_ID = re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]).group(1)\n",
    "                if (DB_CONN_ID <> '*'):\n",
    "                    DBs = DB_CONN_ID.split(',')\n",
    "                else:# * stands for getting all connection name from repository db\n",
    "                    dataframe_dbs = %sql select distinct substr(dbconn_id,1,30) as conn_name from ibm_dsm_views.throughput_all\n",
    "                    dbs = list(dataframe_dbs['CONN_NAME'].values)\n",
    "                    DBs = []\n",
    "                    for id_db in range(len(dbs)):\n",
    "                        DBs.append(str(dbs[id_db]).strip())\n",
    "\n",
    "            # To get the value of INTERVAL\n",
    "            if (re.search(r'INTERVAL\\s?=\\s?([0-9]+$)', all_vars[2]) == None):\n",
    "                err_str = 'The format of INTERVAL error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                INTERVAL = int(re.search(r'(\\d+)', all_vars[2]).group())\n",
    "                if (INTERVAL > 100):  # most get 100 data\n",
    "                    INTERVAL = 100\n",
    "\n",
    "            # To get START_TIME according to END_TIME\n",
    "            if (re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]) == None) | ( re.search(r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]) == None):\n",
    "                err_str = 'The format of END_TIME error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                \n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                END_TIME = re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]).group() + ' ' + re.search( r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]).group()\n",
    "                def is_valid_datetime(END_TIME):\n",
    "                    try:\n",
    "                        time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                        return True\n",
    "                    except:\n",
    "                        return False\n",
    "                \n",
    "                if is_valid_datetime(END_TIME) == True:\n",
    "                    datetime_tuple = time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                    # To slice datetime_tuple to gain exact time data\n",
    "                    year, month, day, hour, minute, second = datetime_tuple[:6]\n",
    "                    final_time = datetime.datetime(year, month, day, hour, minute, second) + datetime.timedelta(hours = -INTERVAL)\n",
    "                    # To transfer the datetime fields to string\n",
    "                    START_TIME = final_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                else:\n",
    "                    err_str = 'The format of END_TIME error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "            # To get the value of REPORT_TYPE\n",
    "            if (re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]) == None):\n",
    "                err_str = 'The format of REPORT_TYPE error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                REPORT_TYPE = re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]).group(1).upper()\n",
    "                if (REPORT_TYPE != 'ALL' and REPORT_TYPE != 'RESOURCE' and REPORT_TYPE != 'CPU' and REPORT_TYPE != 'MEMORY' and REPORT_TYPE != 'IO' and REPORT_TYPE != 'LOG'):\n",
    "                    err_str = 'The format of REPORT_TYPE error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "handle_query_condition()\n",
    "#To print the error of query conditon, only print once\\n\",\n",
    "for err_id in range(len(GENERAL_ERROR)):\n",
    "    print GENERAL_ERROR[err_id]\n",
    "\n",
    "resource_df_empty = False\n",
    "#Get the database string formed by dbname which will be used in sql where cause.\n",
    "db_str = \"\"\n",
    "for id_1 in range(len(DBs)):\n",
    "    if id_1 == len(DBs) - 1:\n",
    "        db_str += \"'\" + DBs[id_1] + \"'\"\n",
    "    else:\n",
    "        db_str += \"'\" + DBs[id_1] + \"'\" + \",\"\n",
    "        \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #dataframe_week = %sql select year(collected) as year, week(collected) as week, substr(dbconn_id,1,30) as conn_name, date(min(collected)) as begin_date, date(max(collected)) as end_date, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(physical_reads_delta) as physical_reads, sum(act_completed_total_delta) as activities, sum(total_app_commits_delta) as commits, sum(total_act_time_delta) / (sum(act_aborted_total_delta) + sum(act_completed_total_delta)) as avg_activity_time_msec from ibm_dsm_views.throughput_all group by year(collected), week(collected), dbconn_id order by year(collected), week(collected), dbconn_id\n",
    "    dataframe_hour = %sql select char(date(collected)) as date, \\\n",
    "                          hour(collected) as hours, \\\n",
    "                          substr(dbconn_id,1,30) as conn_name, \\\n",
    "                          date(min(collected)) as begin_date, \\\n",
    "                          date(max(collected)) as end_date, \\\n",
    "                          sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, \\\n",
    "                          sum(logical_reads_delta) as logical_reads, \\\n",
    "                          sum(physical_reads_delta) as physical_reads, \\\n",
    "                          sum(act_completed_total_delta) as activities, \\\n",
    "                          sum(total_app_commits_delta) as commits, sum(total_act_time_delta) / (sum(act_aborted_total_delta) + sum(act_completed_total_delta)) as avg_activity_time_msec \\\n",
    "                          from ibm_dsm_views.throughput_all \\\n",
    "                          where  dbconn_id in ({db_str}) and collected >= '{START_TIME}' and collected < '{END_TIME}' \\\n",
    "                          group by date(collected), hour(collected), dbconn_id \\\n",
    "                          order by date, hours, dbconn_id\n",
    "    \n",
    "    if os.path.exists(\"resource.csv\"):\n",
    "        !rm resource.csv\n",
    "    if dataframe_hour.empty:\n",
    "        resource_df_empty = True\n",
    "        print 'For resource: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        dataframe_hour.to_csv(\"resource.csv\", index_label = \"INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "   #Declare a list to store all databases' logical reads info\n",
    "    all_logical_read_list = []\n",
    "    #Declare a list to store x ticks\n",
    "    x_ticks = []\n",
    "    #Declare a list to store x ticks' label which is one-t-one match with x ticks\n",
    "    x_ticks_lables = []\n",
    "    #Declare a variable to store the max value of logical reads from all database in repository\n",
    "    y_logical_reads_max = 0\n",
    "    #Get legent title made up with database name\n",
    "    legend_title_logical = []\n",
    "    #Declare a variable to flag logical reads is more than 1 millon\n",
    "    log_mil = False\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, \\\n",
    "        hour(collected) as hours, \\\n",
    "        sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, \\\n",
    "        sum(logical_reads_delta) as logical_reads, \\\n",
    "        sum(PHYSICAL_READS_DELTA) as physical_reads, \\\n",
    "        sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec, \\\n",
    "        sum(act_completed_total_delta) as activities \\\n",
    "        from ibm_dsm_views.throughput_all where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' \\\n",
    "        group by date(collected), hour(collected) \\\n",
    "        order by date, hours\n",
    "        \n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(Logical Reads): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_logical.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_logical_reads = dataframe_hour['LOGICAL_READS']\n",
    "            y_logical_reads_list = list(y_logical_reads)\n",
    "            if (y_logical_reads_max < y_logical_reads.max()):\n",
    "                y_logical_reads_max = y_logical_reads.max()\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = commonUtil.get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_logical_reads_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_logical_reads_list.append(list(y_logical_reads)[tmp_index])\n",
    "                    else:\n",
    "                        y_logical_reads_list.append(0)\n",
    "            all_logical_read_list.append(y_logical_reads_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_logical_read_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_log = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='Logical Reads by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        if y_logical_reads.max() > 1000000:\n",
    "            log_mil = True\n",
    "            for id_4 in range(len(all_logical_read_list)):\n",
    "                for id_4_0 in range(len(all_logical_read_list[id_4])):\n",
    "                    all_logical_read_list[id_4][id_4_0] = all_logical_read_list[id_4][id_4_0]/1000000.0\n",
    "        \n",
    "        xnew_hour = []\n",
    "        ynew_logical_reads = []\n",
    "        for id_5 in range(len(all_logical_read_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_logical_read_list[id_5])):\n",
    "                if(all_logical_read_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_logical_read_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_logical_read_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_logical_read_list[id_5][id_6], '%.1f' % all_logical_read_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_logical_read_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_logical_reads = spline(np.asarray(x_ticks), all_logical_read_list[id_5], xnew_hour)\n",
    "                ynew_logical_reads_list = list(ynew_logical_reads)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_logical_reads_list)):\n",
    "                    if (ynew_logical_reads_list[id_7] < 0.0):\n",
    "                        ynew_logical_reads_list[id_7] = 0.0\n",
    "                ynew_logical_reads = np.asarray(ynew_logical_reads_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_logical_reads, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_logical_reads)\n",
    "               \n",
    "        if (log_mil == True):\n",
    "            pl.ylabel(u'LOGICAL_READS(mil times)', fontsize = 12)\n",
    "        else:\n",
    "            pl.ylabel(u'LOGICAL_READS(times)',fontsize=12)\n",
    "        #pl.legend(labels = ['a', 'b'], loc = 'best')\n",
    "        #Set the legends for the both graphs\n",
    "        box_log = ax_log.get_position()\n",
    "        ax_log.set_position([box_log.x0, box_log.y0 + box_log.height * 0.1, box_log.width, box_log.height * 0.9])\n",
    "        ax_log.legend(legend_title_logical, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_logical))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #Declare a Sketchpad as the first graph\n",
    "    all_physical_read_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    y_physical_reads_max = 0\n",
    "    legend_title_physical = []\n",
    "    phy_mil = False\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, \\\n",
    "                              hour(collected) as hours, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, \\\n",
    "                              sum(logical_reads_delta) as logical_reads, \\\n",
    "                              sum(PHYSICAL_READS_DELTA) as physical_reads, \\\n",
    "                              sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec, \\\n",
    "                              sum(act_completed_total_delta) as activities \\\n",
    "                              from ibm_dsm_views.throughput_all \\\n",
    "                              where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' \\\n",
    "                              group by date(collected), hour(collected) \\\n",
    "                              order by date, hours\n",
    "        \n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(Physical Reads): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_physical.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_physical_reads = dataframe_hour['PHYSICAL_READS']\n",
    "            y_physical_reads_list = list(y_physical_reads)\n",
    "            if (y_physical_reads_max < y_physical_reads.max()):\n",
    "                y_physical_reads_max = y_physical_reads.max()\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = commonUtil.get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_physical_reads_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_physical_reads_list.append(list(y_physical_reads)[tmp_index])\n",
    "                    else:\n",
    "                        y_physical_reads_list.append(0)\n",
    "            all_physical_read_list.append(y_physical_reads_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_physical_read_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_phy = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='Physical Reads by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        if y_physical_reads.max() > 1000000:\n",
    "            phy_mil = True\n",
    "            for id_4 in range(len(all_physical_read_list)):\n",
    "                for id_4_0 in range(len(all_physical_read_list[id_4])):\n",
    "                    all_physical_read_list[id_4][id_4_0] = all_physical_read_list[id_4][id_4_0]/1000000.0\n",
    "        \n",
    "        xnew_hour = []\n",
    "        ynew_physical_reads = []\n",
    "        for id_5 in range(len(all_physical_read_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_physical_read_list[id_5])):\n",
    "                if(all_physical_read_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_physical_read_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_physical_read_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_physical_read_list[id_5][id_6], '%.1f' % all_physical_read_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_physical_read_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_physical_reads = spline(np.asarray(x_ticks), all_physical_read_list[id_5], xnew_hour)\n",
    "                ynew_physical_reads_list = list(ynew_physical_reads)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_physical_reads_list)):\n",
    "                    if (ynew_physical_reads_list[id_7] < 0.0):\n",
    "                        ynew_physical_reads_list[id_7] = 0.0\n",
    "                ynew_physical_reads = np.asarray(ynew_physical_reads_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_physical_reads, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_physical_reads)\n",
    "               \n",
    "        if (phy_mil == True):\n",
    "            pl.ylabel(u'PHYSICAL_READS(mil times)', fontsize = 12)\n",
    "        else:\n",
    "            pl.ylabel(u'PHYSICAL_READS(times)',fontsize=12)\n",
    "        #pl.legend(labels = ['a', 'b'], loc = 'best')\n",
    "        #Set the legends for the both graphs\n",
    "        box_phy = ax_phy.get_position()\n",
    "        ax_phy.set_position([box_phy.x0, box_phy.y0 + box_phy.height * 0.1, box_phy.width, box_phy.height * 0.9])\n",
    "        ax_phy.legend(legend_title_physical, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_physical))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #Declare a Sketchpad as the first graph\n",
    "    all_cpu_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    legend_title_cpu = []\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, \\\n",
    "                              hour(collected) as hours, \\\n",
    "                              sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, \\\n",
    "                              sum(logical_reads_delta) as logical_reads, \\\n",
    "                              sum(PHYSICAL_READS_DELTA) as physical_reads, \\\n",
    "                              sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec, \\\n",
    "                              sum(act_completed_total_delta) as activities \\\n",
    "                              from ibm_dsm_views.throughput_all \\\n",
    "                              where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' \\\n",
    "                              group by date(collected), hour(collected) \\\n",
    "                              order by date, hours\n",
    "        \n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(CPU_SEC): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_cpu.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_cpu = dataframe_hour['CPU_SEC']\n",
    "            y_cpu_list = list(y_cpu)\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = commonUtil.get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_cpu_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_cpu_list.append(list(y_cpu)[tmp_index])\n",
    "                    else:\n",
    "                        y_cpu_list.append(0)\n",
    "            all_cpu_list.append(y_cpu_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_cpu_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_cpu = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='CPU Usage by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        xnew_hour = []\n",
    "        ynew_cpu = []\n",
    "        for id_5 in range(len(all_cpu_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_cpu_list[id_5])):\n",
    "                if(all_cpu_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_cpu_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_cpu_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_cpu_list[id_5][id_6], '%.1f' % all_cpu_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_cpu_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_cpu = spline(np.asarray(x_ticks), all_cpu_list[id_5], xnew_hour)\n",
    "                ynew_cpu_list = list(ynew_cpu)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_cpu_list)):\n",
    "                    if (ynew_cpu_list[id_7] < 0.0):\n",
    "                        ynew_cpu_list[id_7] = 0.0\n",
    "                ynew_cpu = np.asarray(ynew_cpu_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_cpu, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_cpu)\n",
    "               \n",
    "        pl.ylabel(u'CPU_SEC(s)',fontsize=12)\n",
    "        #Set the legends for the both graphs\n",
    "        box_cpu = ax_cpu.get_position()\n",
    "        ax_cpu.set_position([box_cpu.x0, box_cpu.y0 + box_cpu.height * 0.1, box_cpu.width, box_cpu.height * 0.9])\n",
    "        ax_cpu.legend(legend_title_cpu, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_cpu))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'RESOURCE')):\n",
    "    #Declare a Sketchpad as the first graph\n",
    "    all_avg_act_list = []\n",
    "    x_ticks = []\n",
    "    x_ticks_lables = []\n",
    "    legend_title_avg_act = []\n",
    "    for id_1 in range(len(DBs)):\n",
    "        #Get data from the target database to assemble a data frame that will be used for the following graphs\n",
    "        dataframe_hour = %sql select char(date(collected)) as date, \\\n",
    "                              hour(collected) as hours, \\\n",
    "                              sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, \\\n",
    "                              sum(logical_reads_delta) as logical_reads, \\\n",
    "                              sum(PHYSICAL_READS_DELTA) as physical_reads, \\\n",
    "                              sum(total_act_time)/(sum(act_aborted_total)+sum(act_completed_total)) as avg_activity_time_msec, \\\n",
    "                              sum(act_completed_total_delta) as activities \\\n",
    "                              from ibm_dsm_views.throughput_all \\\n",
    "                              where dbconn_id = '{DBs[id_1]}' and collected >= '{START_TIME}' and collected < '{END_TIME}' \\\n",
    "                              group by date(collected), hour(collected) \\\n",
    "                              order by date, hours\n",
    "        \n",
    "        if dataframe_hour.empty:\n",
    "            if resource_df_empty == False:\n",
    "                print 'For resource(AVG_ACTIVITY_TIME_MSEC): The query result is empty for the database connection ' + DBs[id_1] + ', please check your query parameters.\\n'\n",
    "        else:\n",
    "            legend_title_avg_act.append(DBs[id_1])\n",
    "            #Get the hour data as x-axis from the data frame\n",
    "            x_hour = dataframe_hour['HOURS']\n",
    "            y_avg_act = dataframe_hour['AVG_ACTIVITY_TIME_MSEC']\n",
    "            y_avg_act_list = list(y_avg_act)\n",
    "\n",
    "            x_hour_list = list(x_hour) #here x_hour is a python list\n",
    "            date_all = list(dataframe_hour['DATE'].values)\n",
    "\n",
    "            ori_datetime_str = commonUtil.get_original_datatime_str(date_all, x_hour_list)\n",
    "\n",
    "            '''\n",
    "            When the data queried is not equal to the requirement. Filling missing data \n",
    "            into the list x_hour_list,date_all and y_ticks.\n",
    "            '''\n",
    "            if (len(x_hour_list) < INTERVAL):\n",
    "                y_avg_act_list = []\n",
    "                #Transfer datatime data into time tuple for getting its' timestamp\n",
    "                min_dt_str = START_TIME[0:13] + ':00:00' \n",
    "                tm_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "                min_timestamp = time.mktime(tm_tuple)\n",
    "            \n",
    "                date_ref_str = END_TIME[0:13] + ':00:00'\n",
    "                tmp_tuple = time.strptime(date_ref_str, '%Y-%m-%d %H:00:00')\n",
    "                max_timestamp = time.mktime(tmp_tuple)\n",
    "                '''\n",
    "                Get the difference between the max_timestamp and min_timestamp \n",
    "                which will be used for gaining all date and hour including the missing\n",
    "                '''\n",
    "                hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "                #clear date_all\n",
    "                date_all = []\n",
    "                #clear x_hour_list\n",
    "                x_hour_list = []\n",
    "                #clear y_ticks\n",
    "                \n",
    "                #Reassign three var above\n",
    "                for id_2 in range(hour_diff):\n",
    "                    tmp_st = min_timestamp + id_2 * 3600\n",
    "                    #Trans timestamp to datetime string\n",
    "                    tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                    tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    tmp_date_str = tmp_datetime_str[0:10]\n",
    "                    tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                    date_all.append(tmp_date_str)\n",
    "                    x_hour_list.append(tmp_hour_str)\n",
    "                    \n",
    "                    '''\n",
    "                    Below code is for handling y-axis's data\n",
    "                    If there is no data at this hour,0.0 will be filled into\n",
    "                    '''\n",
    "                    if tmp_datetime_str in ori_datetime_str:\n",
    "                        tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                        y_avg_act_list.append(list(y_avg_act)[tmp_index])\n",
    "                    else:\n",
    "                        y_avg_act_list.append(0)\n",
    "            all_avg_act_list.append(y_avg_act_list)\n",
    "           \n",
    "            #previous date string value as a reference data\n",
    "            pre_date_str = str(date_all[0])\n",
    "            \n",
    "            if len(x_ticks) == 0:\n",
    "                for id_3 in range(len(date_all)):\n",
    "                    #Get the data for the x-axis ticks\n",
    "                    x_ticks.append(float('%.1f' % id_3))\n",
    "\n",
    "                    '''\n",
    "                    Get the data for the lable of x-axis\n",
    "                    If the label existed in the list x_ticks_lables,\n",
    "                    put hour_str into x_ticks_lables \n",
    "                    otherwise,put date_lables into x_ticks_lables\n",
    "                    '''\n",
    "                    hour_str = str(x_hour_list[id_3])\n",
    "                    date_str = str(date_all[id_3])\n",
    "                    x_lables = date_str + ' ' + hour_str\n",
    "                    if (id_3 == 0):\n",
    "                        x_ticks_lables.append(x_lables)\n",
    "                    else:#id_3 > 0\n",
    "                        if (pre_date_str == date_str):\n",
    "                            x_ticks_lables.append(hour_str)\n",
    "                        else:\n",
    "                            pre_date_str = date_str\n",
    "                            x_ticks_lables.append(x_lables)\n",
    "                        \n",
    "    if len(all_avg_act_list) > 0:\n",
    "        #Declare a Sketchpad as the first graph\n",
    "        fig = pl.figure()\n",
    "        ax_avg_act = fig.add_subplot(111)\n",
    "        #Declare an ax container as the first drawing paper\n",
    "        x_lable= 'Hours'\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        pl.xlabel(x_lable, fontsize = 12)\n",
    "        #Set title for the second graph\n",
    "        figure_title='Average Activity Time by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        #Set y-axis label\n",
    "        xnew_hour = None\n",
    "        ynew_avg_act = None\n",
    "        for id_5 in range(len(all_avg_act_list)):\n",
    "            #To mark the data point \n",
    "            for id_6 in range(len(all_avg_act_list[id_5])):\n",
    "                if(all_avg_act_list[id_5][id_6] == 0.0):#If no data,drawing a empty circle\n",
    "                    pl.scatter(x_ticks[id_6],all_avg_act_list[id_5][id_6], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:\n",
    "                    pl.scatter(x_ticks[id_6], all_avg_act_list[id_5][id_6], c = '#2c628b') \n",
    "                    pl.text(x_ticks[id_6], all_avg_act_list[id_5][id_6], '%.0f' % all_avg_act_list[id_5][id_6], fontsize = 9)\n",
    "        \n",
    "            if(len(all_avg_act_list[id_5]) >= 3):\n",
    "                ##In order to smooth the line chart,handle the data further##\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_avg_act = spline(np.asarray(x_ticks), all_avg_act_list[id_5], xnew_hour)\n",
    "                ynew_avg_act_list = list(ynew_avg_act)\n",
    "                #No negative value for y-axis\n",
    "                for id_7 in range(len(ynew_avg_act_list)):\n",
    "                    if (ynew_avg_act_list[id_7] < 0.0):\n",
    "                        ynew_avg_act_list[id_7] = 0.0\n",
    "                ynew_avg_act = np.asarray(ynew_avg_act_list)        \n",
    "                #Fill the gragh according to your requirement\n",
    "                #pl.fill_between(xnew_hour, ynew_avg_act, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#2c628b', alpha = 0.09)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_avg_act)\n",
    "               \n",
    "        pl.ylabel(u'AVG_ACTIVITY_TIME_MSEC', fontsize = 12)\n",
    "        #Set the legends for the both graphs\n",
    "        box_avg_act = ax_avg_act.get_position()\n",
    "        ax_avg_act.set_position([box_avg_act.x0, box_avg_act.y0 + box_avg_act.height * 0.1, box_avg_act.width, box_avg_act.height * 0.9])\n",
    "        ax_avg_act.legend(legend_title_avg_act, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_avg_act))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Query condition exists some error\n",
    "if len(GENERAL_ERROR) > 0 and os.path.exists(\"cpu.csv\"):\n",
    "    !rm cpu.csv\n",
    "    \n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'CPU')):\n",
    "    dataframe_cpu = %sql with temp as(select row_number()over(partition by week(collected) \\\n",
    "                         order by sum(total_cpu_usec_delta)/1000000.0 desc) as row_id, \\\n",
    "                         month(collected) as month, \\\n",
    "                         week(collected) as week, \\\n",
    "                         dbconn_id as conn_name, \\\n",
    "                         decimal(sum(total_cpu_usec_delta)/1000000.0,17,2) as cpu_sec \\\n",
    "                         from ibm_dsm_views.throughput_all \\\n",
    "                         group by month(collected), \\\n",
    "                         week(collected), \\\n",
    "                         dbconn_id \\\n",
    "                         order by month, week, row_id) \\\n",
    "                         select month, week, conn_name, cpu_sec, row_id as rank \\\n",
    "                         from temp where row_id <= 5\n",
    "    \n",
    "    if os.path.exists(\"cpu.csv\"):\n",
    "        !rm cpu.csv\n",
    "    if dataframe_cpu.empty:\n",
    "        print 'For CPU: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        dataframe_cpu.to_csv(\"cpu.csv\", index_label = \"INDEX\")\n",
    "        #For the x-labels\n",
    "        x_ticks_labels = list(set(list(dataframe_cpu['WEEK'].values)))\n",
    "        x_ticks = list(range(1, len(x_ticks_labels) + 1))\n",
    "        x_ticks_labels = list(set(list(dataframe_cpu['WEEK'].values)))\n",
    "        #For the y-labels\n",
    "        y_ticks = list(range(1, 6))\n",
    "        y_ticks_labels = list(reversed(y_ticks))\n",
    "        #Get all the connection names from the data frame\n",
    "        all_conn_names = list(dataframe_cpu['CONN_NAME'].values)\n",
    "        #Remove duplicates\n",
    "        distinct_conn_names = list(set(all_conn_names))\n",
    "        #For the size of scatter\n",
    "        conn_times = commonUtil.get_all_times(all_conn_names, distinct_conn_names)\n",
    "        #Get weeks\n",
    "        all_weeks = list(dataframe_cpu['WEEK'].values)\n",
    "        all_ranks = list(dataframe_cpu['RANK'].values)\n",
    "        \n",
    "        fig = pl.figure()\n",
    "        fig.set_size_inches(24,6)\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        #60 colors\n",
    "        all_colors = ['#988FD0', '#E04C32', '#368AB6', '#777777', '#FEBE67', '#66CDAA', '#0000CD', '#BA55D3', '#9370DB', '#3CB371', '#7B68EE', '#00FA9A', '#48D1CC', '#C71585', '#191970', '#F5FFFA', '#FFE4E1', '#FFE4B5', '#FFDEAD', '#000080', '#FDF5E6', '#808000', '#6B8E23', '#FFA500', '#FF4500', '#DA70D6', '#EEE8AA', '#98FB98', '#AFEEEE', '#DB7093', '#FFEFD5', '#FFDAB9', '#CD853F', '#FFC0CB', '#DDA0DD', '#B0E0E6', '#800080', '#FF0000', '#BC8F8F', '#4169E1', '#8B4513', '#FA8072', '#FAA460', '#2E8B57', '#FFF5EE', '#A0522D', '#C0C0C0', '#87CEEB', '#6A5ACD', '#708090', '#FFFAFA', '#00FF7F', '#4682B4', '#D2B48C', '#008080', '#D8BFD8', '#FF6347', '#40E0D0', '#EE82EE', '#F5DEB3']\n",
    "        ax1.set_title('Weekly Top 5 CPU Usage\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax1.set_xlabel('Weeks', fontsize = 12)\n",
    "        ax1.set_ylabel('Rank', fontsize = 12)  \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        ax1.set_xticklabels(x_ticks_labels)\n",
    "        ax1.set_yticks(y_ticks)\n",
    "        ax1.set_yticklabels(y_ticks_labels)\n",
    "        ax1.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        mean_cpu_usage = {}\n",
    "        #Get all the connection names from the data frame\n",
    "        all_cpu_usage = list(dataframe_cpu['CPU_SEC'].values)\n",
    "        \n",
    "        for id_0 in range(len(distinct_conn_names)):\n",
    "            x_data = []\n",
    "            y_data = []\n",
    "            cnt = 0\n",
    "            sum_cpu_usage = 0.0\n",
    "            for id_1 in range(len(all_conn_names)):\n",
    "                if(distinct_conn_names[id_0] == all_conn_names[id_1]):\n",
    "                    cnt += 1\n",
    "                    sum_cpu_usage += all_cpu_usage[id_1]\n",
    "                    x_data.append(x_ticks_labels.index(all_weeks[id_1]) + 1)\n",
    "                    y_data.append(6 - all_ranks[id_1])\n",
    "                \n",
    "            mean_cpu_usage[str(distinct_conn_names[id_0])] = float('%.1f' % (sum_cpu_usage / cnt))\n",
    "            ax1.scatter(x_data, y_data, c = all_colors[id_0], marker = 'o', edgecolors = 'r', s = 200 * conn_times[distinct_conn_names[id_0]])\n",
    "            ax1.text(x_data[0], y_data[0], '%s' % distinct_conn_names[id_0], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'right')\n",
    "            ax1.plot(x_data, y_data, linestyle = '--', color = all_colors[id_0], lw = 1.0)\n",
    "        \n",
    "        #Below is the second graph\n",
    "        ax2.set_title('Top CPU Usage Map\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax2.set_xlabel('Frequency', fontsize = 12)\n",
    "        ax2.set_ylabel('Rank', fontsize = 12)\n",
    "        max_times = commonUtil.getmaxtime(conn_times)\n",
    "        x_ticks = list(range(1, max_times + 1))\n",
    "        x_data = conn_times.values()\n",
    "        y_data = commonUtil.get_mean_rank(all_conn_names, distinct_conn_names, all_ranks)\n",
    "        circle_size = commonUtil.get_sort_circle(mean_cpu_usage)\n",
    "        ax2.set_xticks(x_ticks)\n",
    "        ax2.set_xticklabels(x_ticks)\n",
    "        ax2.set_yticks(y_ticks)\n",
    "        ax2.set_yticklabels(y_ticks_labels)\n",
    "        ax2.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        for ax2_idx in range(len(distinct_conn_names)):\n",
    "            ax2.scatter(x_data[ax2_idx], y_data[ax2_idx], c = all_colors[ax2_idx], marker = 'o', edgecolors = 'r', s = 200 * circle_size[distinct_conn_names[ax2_idx]])\n",
    "            ax2.text(x_data[ax2_idx], y_data[ax2_idx], '%s' % distinct_conn_names[ax2_idx], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'left')\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Query condition exists some error\n",
    "if len(GENERAL_ERROR) > 0 and os.path.exists(\"memory.csv\"):\n",
    "    !rm memory.csv\n",
    "\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'MEMORY')):\n",
    "    dataframe_memory = %sql with temp as(select row_number()over(partition by week(collected) \\\n",
    "                            order by max(memory_pool_used_gb) desc) as row_id, \\\n",
    "                            month(collected) as month, \\\n",
    "                            week(collected) as week, \\\n",
    "                            dbconn_id as conn_name, \\\n",
    "                            dec(max(memory_pool_used_gb),17,2) as memory_usage_gb \\\n",
    "                            from IBM_DSM_VIEWS.MEM_DB_TOTAL_USED \\\n",
    "                            group by month(collected), \\\n",
    "                            week(collected), \\\n",
    "                            dbconn_id \\\n",
    "                            order by month, week, row_id) \\\n",
    "                            select month, week, conn_name, memory_usage_gb, row_id as rank \\\n",
    "                            from temp where row_id < 5\n",
    "    \n",
    "    if os.path.exists(\"memory.csv\"):\n",
    "        !rm memory.csv\n",
    "    if dataframe_memory.empty:\n",
    "        print 'For Memory: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        dataframe_memory.to_csv(\"memory.csv\", index_label = \"INDEX\")\n",
    "        #For the x-labels\n",
    "        x_ticks_labels = list(set(list(dataframe_memory['WEEK'].values)))\n",
    "        x_ticks = list(range(1, len(x_ticks_labels) + 1))\n",
    "        x_ticks_labels = list(set(list(dataframe_memory['WEEK'].values)))\n",
    "        #For the y-labels\n",
    "        y_ticks = list(range(1, 6))\n",
    "        y_ticks_labels = list(reversed(y_ticks))\n",
    "        #Get all the connection names from the data frame\n",
    "        all_conn_names = list(dataframe_memory['CONN_NAME'].values)\n",
    "        #Remove duplicates\n",
    "        distinct_conn_names = list(set(all_conn_names))\n",
    "        #For the size of scatter\n",
    "        conn_times = commonUtil.get_all_times(all_conn_names, distinct_conn_names)\n",
    "        #Get weeks\n",
    "        all_weeks = list(dataframe_memory['WEEK'].values)\n",
    "        all_ranks = list(dataframe_memory['RANK'].values)\n",
    "        fig = pl.figure()\n",
    "        fig.set_size_inches(24,6)\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        #60 colors\n",
    "        all_colors = ['#988FD0', '#E04C32', '#368AB6', '#777777', '#FEBE67', '#66CDAA', '#0000CD', '#BA55D3', '#9370DB', '#3CB371', '#7B68EE', '#00FA9A', '#48D1CC', '#C71585', '#191970', '#F5FFFA', '#FFE4E1', '#FFE4B5', '#FFDEAD', '#000080', '#FDF5E6', '#808000', '#6B8E23', '#FFA500', '#FF4500', '#DA70D6', '#EEE8AA', '#98FB98', '#AFEEEE', '#DB7093', '#FFEFD5', '#FFDAB9', '#CD853F', '#FFC0CB', '#DDA0DD', '#B0E0E6', '#800080', '#FF0000', '#BC8F8F', '#4169E1', '#8B4513', '#FA8072', '#FAA460', '#2E8B57', '#FFF5EE', '#A0522D', '#C0C0C0', '#87CEEB', '#6A5ACD', '#708090', '#FFFAFA', '#00FF7F', '#4682B4', '#D2B48C', '#008080', '#D8BFD8', '#FF6347', '#40E0D0', '#EE82EE', '#F5DEB3']\n",
    "        ax1.set_title('Weekly Top 5 Memory Usage\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax1.set_xlabel('Weeks', fontsize = 12)\n",
    "        ax1.set_ylabel('Rank', fontsize = 12)  \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        ax1.set_xticklabels(x_ticks_labels)\n",
    "        ax1.set_yticks(y_ticks)\n",
    "        ax1.set_yticklabels(y_ticks_labels)\n",
    "        ax1.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        \n",
    "        mean_memory_usage = {}\n",
    "        #Get all the connection names from the data frame\n",
    "        all_memory_usage = list(dataframe_memory['MEMORY_USAGE_GB'].values)\n",
    "\n",
    "        for id_0 in range(len(distinct_conn_names)):\n",
    "            x_data = []\n",
    "            y_data = []\n",
    "            cnt = 0\n",
    "            sum_memory_usage = 0.0\n",
    "            for id_1 in range(len(all_conn_names)):\n",
    "                if(distinct_conn_names[id_0] == all_conn_names[id_1]):\n",
    "                    cnt += 1\n",
    "                    sum_memory_usage += all_memory_usage[id_1]\n",
    "                    x_data.append(x_ticks_labels.index(all_weeks[id_1]) + 1)\n",
    "                    y_data.append(6 - all_ranks[id_1])\n",
    "            mean_memory_usage[str(distinct_conn_names[id_0])] = float('%.1f' % (sum_memory_usage / cnt))\n",
    "            ax1.scatter(x_data, y_data, c = all_colors[id_0], marker = 'o', edgecolors = 'r', s = 200 * conn_times[distinct_conn_names[id_0]])\n",
    "            ax1.text(x_data[0], y_data[0], '%s' % distinct_conn_names[id_0], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'right')\n",
    "            ax1.plot(x_data, y_data, linestyle = '--', color = all_colors[id_0], lw = 1.0)\n",
    "        \n",
    "        #Below is the second graph\n",
    "        ax2.set_title('Top Memory Usage Map\\n',fontsize = 14, fontweight = 'bold')  \n",
    "        ax2.set_xlabel('Frequency', fontsize = 12)\n",
    "        ax2.set_ylabel('Rank', fontsize = 12)\n",
    "        max_times = commonUtil.getmaxtime(conn_times)\n",
    "        x_ticks = list(range(1, max_times + 1))\n",
    "        x_data = conn_times.values()\n",
    "        y_data = commonUtil.get_mean_rank(all_conn_names, distinct_conn_names, all_ranks)\n",
    "        circle_size = commonUtil.get_sort_circle(mean_memory_usage)\n",
    "        ax2.set_xticks(x_ticks)\n",
    "        ax2.set_xticklabels(x_ticks)\n",
    "        ax2.set_yticks(y_ticks)\n",
    "        ax2.set_yticklabels(y_ticks_labels)\n",
    "        ax2.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        for ax2_idx in range(len(distinct_conn_names)):\n",
    "            ax2.scatter(x_data[ax2_idx], y_data[ax2_idx], c = all_colors[ax2_idx], marker = 'o', edgecolors = 'r', s = 200 * circle_size[distinct_conn_names[ax2_idx]])\n",
    "            ax2.text(x_data[ax2_idx], y_data[ax2_idx], '%s' % distinct_conn_names[ax2_idx], fontsize = 9, verticalalignment = 'top', horizontalalignment = 'left')\n",
    "        pl.show()       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
