{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the required lib\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "'''\n",
    "To get query condition string via reading the file:logCondition\n",
    "and handle it to gain all parameters by split function. All parameters\n",
    "will be stored in below global variables\n",
    "'''\n",
    "DB_CONN_ID = ''\n",
    "END_TIME = ''\n",
    "INTERVAL = 0\n",
    "START_TIME = ''\n",
    "REPORT_TYPE = ''\n",
    "ISOK_ALL_PARA = 1\n",
    "GENERAL_ERROR = None\n",
    "\n",
    "def handle_query_condition():\n",
    "    # To declare below variables are global variables\n",
    "    global DB_CONN_ID\n",
    "    global END_TIME\n",
    "    global INTERVAL\n",
    "    global START_TIME\n",
    "    global REPORT_TYPE\n",
    "    global ISOK_ALL_PARA\n",
    "    global GENERAL_ERROR\n",
    "    \n",
    "    # To read parameters from singleDBCondition file\n",
    "    where_cause = !cat logCondition\n",
    "    where_cause = re.search(r'DB_CONN_ID\\s?=\\s?.+END_TIME\\s?=\\s?.+INTERVAL\\s?=\\s?.+REPORT_TYPE\\s?=\\s?.+', where_cause[0])\n",
    "    GENERAL_ERROR = []\n",
    "    \n",
    "    if (where_cause <> None):\n",
    "        all_vars = where_cause.group().split(' ')\n",
    "        if len(all_vars) <> 4:\n",
    "            err_str = 'The format of parameters is error.'\n",
    "            if err_str not in GENERAL_ERROR:\n",
    "                GENERAL_ERROR.append(err_str)\n",
    "            ISOK_ALL_PARA = 0\n",
    "        else:# len(all_vars) == 4\n",
    "            # To get DB_CONN_ID\n",
    "            if (re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]) == None):\n",
    "                err_str = 'The DB_CONN_ID can not be empty.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                DB_CONN_ID = re.search(r'DB_CONN_ID\\s?=\\s?(.+)', all_vars[0]).group(1)\n",
    "\n",
    "            # To get the value of INTERVAL\n",
    "            if (re.search(r'INTERVAL\\s?=\\s?([0-9]+$)', all_vars[2]) == None):\n",
    "                err_str = 'The format of INTERVAL error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                INTERVAL = int(re.search(r'(\\d+)', all_vars[2]).group())\n",
    "                if (INTERVAL > 100):# most get 100 data\n",
    "                    INTERVAL = 100\n",
    "\n",
    "            # To get START_TIME according to END_TIME\n",
    "            if (re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]) == None) | ( re.search(r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]) == None):\n",
    "                err_str = 'The format of END_TIME error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                END_TIME = re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2})', all_vars[1]).group() + ' ' + re.search( r'(\\d{1,2}:\\d{1,2}:\\d{1,2})', all_vars[1]).group()\n",
    "                def is_valid_datetime(END_TIME):\n",
    "                    try:\n",
    "                        time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                        return True\n",
    "                    except:\n",
    "                        return False\n",
    "                \n",
    "                if is_valid_datetime(END_TIME) == True:\n",
    "                    datetime_tuple = time.strptime(END_TIME, '%Y-%m-%d %H:%M:%S')\n",
    "                    # To slice datetime_tuple to gain exact time data\n",
    "                    year, month, day, hour, minute, second = datetime_tuple[:6]\n",
    "                    final_time = datetime.datetime(year, month, day, hour, minute, second) + datetime.timedelta(hours = -INTERVAL)\n",
    "                    # To transfer the datetime fields to string\n",
    "                    START_TIME = final_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                else:\n",
    "                    err_str = 'The format of END_TIME error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "\n",
    "            # To get the value of REPORT_TYPE\n",
    "            if (re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]) == None):\n",
    "                err_str = 'The format of REPORT_TYPE error.'\n",
    "                if err_str not in GENERAL_ERROR:\n",
    "                    GENERAL_ERROR.append(err_str)\n",
    "                ISOK_ALL_PARA = 0\n",
    "            else:\n",
    "                REPORT_TYPE = re.search(r'REPORT_TYPE\\s?=\\s?(.+)', all_vars[3]).group(1).upper()\n",
    "                if (REPORT_TYPE != 'ALL' and REPORT_TYPE != 'RESOURCE' and REPORT_TYPE != 'CPU' and REPORT_TYPE != 'MEMORY' and REPORT_TYPE != 'LOG'):\n",
    "                    err_str = 'The format of REPORT_TYPE error.'\n",
    "                    if err_str not in GENERAL_ERROR:\n",
    "                        GENERAL_ERROR.append(err_str)\n",
    "                    ISOK_ALL_PARA = 0\n",
    "    else:\n",
    "        err_str = 'The format of parameters is error11.'\n",
    "        if err_str not in GENERAL_ERROR:\n",
    "            GENERAL_ERROR.append(err_str)\n",
    "        ISOK_ALL_PARA = 0\n",
    "\n",
    "'''\n",
    "This function is used to get all datatime string according to the queried result \n",
    "from database.\n",
    "\n",
    "The variable ori_datetime_str is a dictionary and its data \n",
    "from the combination of hour_list and date_all, which will \n",
    "be used for judging whether some data exists in it or not.\n",
    "'''\n",
    "def get_original_datatime_str(date_all, hour_list):\n",
    "    ori_datetime_str = {}\n",
    "    for indx in range(len(hour_list)):\n",
    "        tmp_date_str = date_all[indx].encode('unicode-escape').decode('string_escape')\n",
    "        tmp_hour_str = str(hour_list[indx])\n",
    "        tmp_datetime_str = ''\n",
    "        if(len(tmp_hour_str) == 1):#Change 1:00:00 into 01:00:00\n",
    "            tmp_hour_str = '0' + tmp_hour_str\n",
    "        tmp_datetime_str = tmp_date_str + ' ' + tmp_hour_str + ':00:00'\n",
    "        ori_datetime_str[tmp_datetime_str] = indx\n",
    "    return ori_datetime_str\n",
    "\n",
    "'''\n",
    "This function is used to get x_ticks and x_ticks_lables.\n",
    "x_ticks: the scale of x-axis,sorted by asc\n",
    "x_ticks_lables: the label for each points of x-axis \n",
    "'''\n",
    "def format_x_axis(date_all, hour_list, x_ticks, x_ticks_lables):\n",
    "    #To store previous date string value as a reference data\n",
    "    pre_date_str = str(date_all[0])\n",
    "\n",
    "    for dateIdx in range(len(date_all)):\n",
    "        # To get the data for the x-axis ticks\n",
    "        x_ticks.append(float('%0.1f' % dateIdx))\n",
    "        '''\n",
    "        Get the data for the label of x-axis\n",
    "        Each day only store once,and the final label as following:\n",
    "        2018-01-02 02---only once 2018-01-02\n",
    "                   03\n",
    "                   04\n",
    "                   ...\n",
    "        '''\n",
    "        hour_str = str(hour_list[dateIdx])\n",
    "        if len(hour_str) == 1:\n",
    "            hour_str = '0' + hour_str\n",
    "        date_str = str(date_all[dateIdx])\n",
    "        x_lables = date_str + ' ' + hour_str\n",
    "        if (dateIdx == 0):\n",
    "            x_ticks_lables.append(x_lables)\n",
    "        else:#dateIdx > 0\n",
    "            if (pre_date_str == date_str):\n",
    "                x_ticks_lables.append(hour_str)\n",
    "            else:\n",
    "                pre_date_str = date_str\n",
    "                x_ticks_lables.append(x_lables)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the required lib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.interpolate import spline\n",
    "from sklearn import linear_model\n",
    "\n",
    "#To get all query conditions\n",
    "handle_query_condition()\n",
    "\n",
    "'''\n",
    "According to original dataframe we can get a serial data set \n",
    "and use it to form a new dataframe.\n",
    "'''\n",
    "def get_train_df(dataframe_hour):\n",
    "    try:\n",
    "        #Get hour from original dataframe\n",
    "        x_hour_list = list(dataframe_hour['HOURS'].values)\n",
    "        #Get date from original dataframe\n",
    "        date_ori_list = list(dataframe_hour['DATE'].values)\n",
    "        #Get y_max_log_percent from original dataframe\n",
    "        y_max_log_percent = list(dataframe_hour['ACTIVE_LOG_USED_PERCENT_MAX'].values)\n",
    "        #Get y_avg_log_percent from original dataframe\n",
    "        y_avg_log_percent = list(dataframe_hour['ACTIVE_LOG_USED_PERCENT_AVG'].values)\n",
    "        \n",
    "        ##Get timestamp using hour and date from original dataframe\n",
    "        ts_list = []\n",
    "        for i in range(len(x_hour_list)):\n",
    "            hr_str = str(x_hour_list[i])\n",
    "            if len(hr_str) == 1:\n",
    "                hr_str = '0' + hr_str + ':00:00'\n",
    "            else:\n",
    "                hr_str = hr_str + ':00:00'\n",
    "            dt_str = str(date_ori_list[i]) + ' ' + hr_str\n",
    "            time_tuple = time.strptime(dt_str, '%Y-%m-%d %H:00:00')\n",
    "            ts = int(time.mktime(time_tuple))\n",
    "            ts_list.append(ts)\n",
    "        '''\n",
    "        Judge whether data set is serial or not\n",
    "        '''  \n",
    "        index_list = []\n",
    "        tmp_index = []\n",
    "        for j in range(len(ts_list) - 1):\n",
    "            if j not in tmp_index:\n",
    "                tmp_index.append(j)\n",
    "            if (ts_list[j + 1] - ts_list[j]) == 3600:\n",
    "                tmp_index.append(j + 1)\n",
    "                if (j + 1) == len(ts_list) - 1:\n",
    "                    index_list.append(tmp_index)\n",
    "                    tmp_index = None\n",
    "            else:\n",
    "                index_list.append(tmp_index)\n",
    "                tmp_index = []\n",
    "                if (j + 1) == len(ts_list) - 1:\n",
    "                    tmp_index.append(j + 1)\n",
    "                    index_list.append(tmp_index)\n",
    "                    tmp_index = []\n",
    "        max_continuous_index = []\n",
    "        \n",
    "        #for l in range(len(index_list)):\n",
    "            #max_len = len(index_list[0])\n",
    "            #if max_len <= len(index_list[l]):\n",
    "                #max_len = len(index_list[l])\n",
    "                #max_continuous_index = index_list[l]\n",
    "        \n",
    "        #Get the newest continuous data set\n",
    "        from_end_len = len(index_list) - 1\n",
    "        while from_end_len >= 0:\n",
    "            if(len(index_list[from_end_len]) >= 100):\n",
    "                max_continuous_index = index_list[len(index_list) - 1]\n",
    "                break\n",
    "            else:\n",
    "                if from_end_len == 0:\n",
    "                    max_continuous_index = index_list[len(index_list) - 1]\n",
    "                from_end_len_ -= 1\n",
    "        x_hour_list = x_hour_list[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        date_ori_list = date_ori_list[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        y_max_log_percent = y_max_log_percent[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        y_avg_log_percent = y_avg_log_percent[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        \n",
    "        df = pd.DataFrame()#[date_ori_list, x_hour_list, y_max_log_percent, y_avg_log_percent], columns=['DATE', 'HOURS', 'ACTIVE_LOG_USED_PERCENT_MAX', 'ACTIVE_LOG_USED_PERCENT_AVG'])\n",
    "        df['DATE'] = pd.Series(date_ori_list)\n",
    "        df['HOURS'] = pd.Series(x_hour_list)\n",
    "        df['ACTIVE_LOG_USED_PERCENT_MAX'] = pd.Series(y_max_log_percent)\n",
    "        df['ACTIVE_LOG_USED_PERCENT_AVG'] = pd.Series(y_avg_log_percent)\n",
    "        return df\n",
    "    except Exception as ex:\n",
    "        print str(ex)\n",
    "#To print the error of query conditon, only print once\n",
    "for err_id in range(len(GENERAL_ERROR)):\n",
    "    print GENERAL_ERROR[err_id]\n",
    "    \n",
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'LOG')):\n",
    "    #Get data from the target database to assemble a data frame that will be used for the following graph\n",
    "    dataframe_interval = %sql SELECT char(date(COLLECTED)) as DATE, hour(COLLECTED) as HOURS, decimal(round(avg(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_AVG_KB, decimal(round(max(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_MAX_KB, decimal(round(avg(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_AVG_KB, decimal(round(max(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_MAX_KB, decimal(round(avg(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_AVG, decimal(round(max(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_MAX, decimal(round(avg(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_AVG_KB, decimal(round(max(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_MAX_KB, decimal(round(avg(decimal(SEC_LOG_USED_TOP, 17, 2))/1024, 2), 17, 2) as SEC_LOG_USED_TOP_AVG_KB, decimal(round(max(decimal(SEC_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as SEC_LOG_USED_TOP_MAX_KB, avg(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_AVG, max(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_MAX from IBM_DSM_VIEWS.MON_GET_TRANSACTION_LOG where dbconn_id='{DB_CONN_ID}' and collected >= '{START_TIME}' and collected < '{END_TIME}' group by date(COLLECTED), hour(COLLECTED) order by DATE,HOURS\n",
    "    if dataframe_interval.empty:\n",
    "        print 'For Log: The query result is empty, please check your query parameters.\\n'\n",
    "        if os.path.exists(\"log.csv\"):\n",
    "            !rm log.csv\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        if os.path.exists(\"log.csv\"):\n",
    "            !rm log.csv\n",
    "        dataframe_interval.to_csv(\"log.csv\", index_label = \"INDEX\")\n",
    "        \n",
    "        dataframe_hour_train = %sql SELECT char(date(COLLECTED)) as DATE, hour(COLLECTED) as HOURS, decimal(round(avg(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_AVG_KB, decimal(round(max(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_MAX_KB, decimal(round(avg(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_AVG_KB, decimal(round(max(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_MAX_KB, decimal(round(avg(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_AVG, decimal(round(max(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_MAX, decimal(round(avg(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_AVG_KB, decimal(round(max(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_MAX_KB, decimal(round(avg(decimal(SEC_LOG_USED_TOP, 17, 2))/1024, 2), 17, 2) as SEC_LOG_USED_TOP_AVG_KB, decimal(round(max(decimal(SEC_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as SEC_LOG_USED_TOP_MAX_KB, avg(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_AVG, max(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_MAX from IBM_DSM_VIEWS.MON_GET_TRANSACTION_LOG where dbconn_id='{DB_CONN_ID}' and collected < '{END_TIME}' group by date(COLLECTED), hour(COLLECTED) order by DATE,HOURS\n",
    "        #Get continuous data to build a predict model\n",
    "        df_train = get_train_df(dataframe_hour_train)\n",
    "        \n",
    "        #Get the hour data as x-axis from the data frame and convert the data into a numpy array\n",
    "        x_hour_list_train = list(df_train['HOURS'].values)\n",
    "        date_ori_train = list(df_train['DATE'].values)\n",
    "        #Get the data as y-axis from the data frame and convert the data into a numpy array\n",
    "        y_max_log_percent_list_train = list(df_train['ACTIVE_LOG_USED_PERCENT_MAX'].values)\n",
    "        #Get the data as y-axis from the data frame and convert the data into a numpy array\n",
    "        y_avg_log_percent_list_train = list(df_train['ACTIVE_LOG_USED_PERCENT_AVG'].values)\n",
    "        \n",
    "        #Predict INTERVAL points\n",
    "        predict_count = float('%.1f' % INTERVAL)\n",
    "\n",
    "        ######################\n",
    "        ## create predictive model with time series algorithm ARIMA(Autoregressiveâ€“moving-average model)\n",
    "        ## For the source data are non-seasonal, ARIMA(p, d, q) instead of ARIMA(p, d, q, s) is used to build model\n",
    "        ## ARIMA(p, d, q): \n",
    "        ##     AR(p) represents the P order autoregressive process; \n",
    "        ##     MA(q) represents the moving average process of the Q order; \n",
    "        ##     d respresents the number of difference\n",
    "        ## Refer to notebook ARIMA-Analysis for how to determine number of p, d, q with detail\n",
    "        \n",
    "        ## Generate pd.Series object for Time Series analysis \n",
    "        df_max = pd.Series(y_max_log_percent_list_train)\n",
    "        df_avg = pd.Series(y_avg_log_percent_list_train)\n",
    "        \n",
    "        datetime_str_train = []\n",
    "        for id_1 in range(len(date_ori_train)):\n",
    "            tmp_hour = str(x_hour_list_train[id_1])\n",
    "            if(len(tmp_hour) == 1):\n",
    "                tmp_hour = '0' + tmp_hour\n",
    "            tmp_dt = str(date_ori_train[id_1]) + ' ' + tmp_hour\n",
    "            datetime_str_train.append(tmp_dt)\n",
    "        ## \n",
    "        df_max.index = pd.PeriodIndex(start = datetime_str_train[0], end = datetime_str_train[len(datetime_str_train) - 1])\n",
    "        df_avg.index = pd.PeriodIndex(start = datetime_str_train[0], end = datetime_str_train[len(datetime_str_train) - 1])\n",
    "        \n",
    "        ## Apply data to ARIMA algorithm to generate Time Series analysis model with forcast data\n",
    "        model_max = sm.tsa.ARIMA(df_max,(7, 0, 1)).fit()\n",
    "        model_avg = sm.tsa.ARIMA(df_avg,(7, 0, 1)).fit()\n",
    "        \n",
    "        ## Handling interval\n",
    "        ## interval<=10, shows number of real data and predictive data same to interval\n",
    "        ## interval>10, shows number of real data same to interval and 10 predictive data\n",
    "   \n",
    "        #Define a empty var x_ticks to store x-axis ticks(marks)\n",
    "        x_ticks = []\n",
    "        #Define a empty var x_ticks_lables to restore x-axis labels\n",
    "        x_ticks_lables = []\n",
    "        #Define a empty var y_ticks to store y-axis ticks(marks)\n",
    "        y_max_log_percent_list = list(dataframe_interval['ACTIVE_LOG_USED_PERCENT_MAX'].values)\n",
    "        y_avg_log_percent_list = list(dataframe_interval['ACTIVE_LOG_USED_PERCENT_AVG'].values)\n",
    "        \n",
    "        end_dt_str = END_TIME[0:13] + \":00:00\"\n",
    "        end_tuple = time.strptime(end_dt_str, '%Y-%m-%d %H:00:00')\n",
    "        end_timestamp = time.mktime(end_tuple)\n",
    "        \n",
    "        previous_hour = None\n",
    "        predict_hour = None\n",
    "        aft_hour_st = None\n",
    "        pre_hour_st = end_timestamp - 3600.0 * INTERVAL\n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(pre_hour_st)\n",
    "        previous_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "        \n",
    "        aft_hour_st = end_timestamp + 3600 * predict_count\n",
    "            \n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(aft_hour_st)\n",
    "        predict_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "        \n",
    "        ## Define format of x-Axis\n",
    "        date_all = []\n",
    "        x_hour = []\n",
    "        loop_count = int((aft_hour_st - pre_hour_st)/3600.0)\n",
    "        for id_2 in range(loop_count):\n",
    "            tmp_hour_st = pre_hour_st + id_2 * 3600.0\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(tmp_hour_st)\n",
    "            date_all.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:10])\n",
    "            x_hour.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[11:13])\n",
    "        format_x_axis(date_all, x_hour, x_ticks, x_ticks_lables)\n",
    "        ## Apply data to model to generate forcast data\n",
    "        predict_array_max = model_max.predict(datetime_str_train[len(datetime_str_train) - 1], predict_hour, dynamic=True)\n",
    "        #print 'predict_array_max=',predict_array_max\n",
    "        #predict_array_max_list = list(predict_array_max)\n",
    "        predict_array_max = predict_array_max[1:INTERVAL + 1]\n",
    "        ## Prepare data for Y-Axis\n",
    "        y_max_log_percent_list = y_max_log_percent_list[-INTERVAL:] + list(predict_array_max)\n",
    "        \n",
    "        ## Apply data to model to generate forcast data\n",
    "        predict_array_avg = model_avg.predict(datetime_str_train[len(datetime_str_train) - 1], predict_hour, dynamic=True)\n",
    "        predict_array_avg =  predict_array_max = predict_array_avg[1:INTERVAL + 1]\n",
    "        y_avg_log_percent_list = y_avg_log_percent_list[-INTERVAL:] + list(predict_array_avg)\n",
    "        \n",
    "        #ax = df_final.ix[previous_ten_hour:].plot(ax=ax)\n",
    "        #predict_sunspots.plot(ax=ax)\n",
    "        fig, ax = pl.subplots(figsize=(24, 7))\n",
    "        figure_title = 'Max and Average Log Usage Percentage by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        x_lable = 'Hours'\n",
    "        #To set x-axis label\n",
    "        pl.xlabel(x_lable)\n",
    "        #To set y-axis label\n",
    "        pl.ylabel(u'ACTIVE_LOG_USED_PERCENT %')\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        \n",
    "        ##To scatter the date of y_max_log_percent_list and mark the data point \n",
    "        for id in range(0,len(y_max_log_percent_list),2):\n",
    "            if(y_max_log_percent_list[id] == 0.0):#If no data,drawing a empty circle\n",
    "                if id >= len(y_max_log_percent_list) - predict_count and id < len(y_max_log_percent_list):\n",
    "                    #This is the data of prediction and empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = 'r') \n",
    "                else:\n",
    "                    #This is the real data of y_max_log_percent and empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "            else:#Data not empty\n",
    "                if id >= len(y_max_log_percent_list) - predict_count and id < len(y_max_log_percent_list):\n",
    "                    #This is the data of prediction and not empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = 'r') \n",
    "                    pl.text(x_ticks[id], y_max_log_percent_list[id], '%.1f' % y_max_log_percent_list[id], fontsize = 9)\n",
    "                else:\n",
    "                    #This is the real data of y_max_log_percent and not empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = '#60bdae') \n",
    "                    pl.text(x_ticks[id], y_max_log_percent_list[id], '%.1f' % y_max_log_percent_list[id], fontsize = 9)\n",
    "        \n",
    "        #### smooth the lines\n",
    "        for id in range(1, len(y_avg_log_percent_list), 2):\n",
    "            if(y_avg_log_percent_list[id] == 0.0):#If no data,drawing a empty circle\n",
    "                if id >= len(y_avg_log_percent_list) - predict_count and id < len(y_avg_log_percent_list):\n",
    "                    #This is the data of prediction and empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = 'r') \n",
    "                else:\n",
    "                    #This is the real data of y_avg_log_percent and empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "            else:#Data not empty\n",
    "                if id >= len(y_avg_log_percent_list) - predict_count and id < len(y_avg_log_percent_list):\n",
    "                    #This is the data of prediction and not empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = 'r') \n",
    "                    pl.text(x_ticks[id], y_avg_log_percent_list[id], '%.1f' % y_avg_log_percent_list[id], fontsize = 9)\n",
    "                else:\n",
    "                    #This is the real data of y_avg_log_percent and not empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = '#60bdae') \n",
    "                    pl.text(x_ticks[id], y_avg_log_percent_list[id], '%.1f' % y_avg_log_percent_list[id], fontsize = 9)\n",
    "        \n",
    "        xnew_hour = []\n",
    "        ynew_max_log_percent = []\n",
    "        ynew_avg_log_percent = []\n",
    "        \n",
    "        ##In order to smooth the line chart,handle the data further##\n",
    "        #Expand each x axis data 20 times\n",
    "        xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "        #Handle the data of new y axis data\n",
    "        ynew_max_log_percent = spline(np.asarray(x_ticks), np.asarray(y_max_log_percent_list), xnew_hour)\n",
    "        ynew_max_log_percent_list = list(ynew_max_log_percent)\n",
    "        \n",
    "        ynew_avg_log_percent = spline(np.asarray(x_ticks), np.asarray(y_avg_log_percent_list), xnew_hour)\n",
    "        ynew_avg_log_percent_list = list(ynew_avg_log_percent)\n",
    "        #No negative value for y-axis\n",
    "        for y_idx in range(len(ynew_max_log_percent_list)):\n",
    "            if (ynew_max_log_percent_list[y_idx] < 0.0):\n",
    "                ynew_max_log_percent_list[y_idx] = 0.0\n",
    "        ynew_max_log_percent = np.asarray(ynew_max_log_percent_list)\n",
    "        \n",
    "        #No negative value for y-axis\n",
    "        for y_idx in range(len(ynew_avg_log_percent_list)):\n",
    "            if (ynew_avg_log_percent_list[y_idx] < 0.0):\n",
    "                ynew_avg_log_percent_list[y_idx] = 0.0\n",
    "        ynew_avg_log_percent = np.asarray(ynew_avg_log_percent_list)\n",
    "\n",
    "        #Fill the gragh according to your requirement  tian chong yin ying\n",
    "        max_ratio = len(list(predict_array_max)) / float('%.1f' % len(y_max_log_percent_list))\n",
    "        pl.fill_between(xnew_hour, ynew_max_log_percent, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max() * (1.0 - max_ratio)), color = '#60bdae', alpha = 0.15)\n",
    "        pl.fill_between(xnew_hour, ynew_max_log_percent, where=(xnew_hour.max() * (1.0 - max_ratio) <xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#60bdae', alpha = 0.5)\n",
    "        #Draw curve graph\n",
    "        pl.plot(xnew_hour, ynew_max_log_percent, color = '#60bdae')\n",
    "        \n",
    "        #Fill the gragh according to your requirement\n",
    "        avg_ratio = len(list(predict_array_avg)) / float('%.1f' % len(y_avg_log_percent_list))\n",
    "        pl.fill_between(xnew_hour, ynew_avg_log_percent, where=(xnew_hour.min() < xnew_hour) & (xnew_hour < xnew_hour.max() * (1.0 - avg_ratio)), color = '#4c78fb', alpha = 0.15)\n",
    "        pl.fill_between(xnew_hour, ynew_avg_log_percent, where=(xnew_hour.max() * (1.0 - avg_ratio) < xnew_hour) & (xnew_hour < xnew_hour.max()), color = '#4c78fb', alpha = 0.5)\n",
    "        #Draw curve graph\n",
    "        pl.plot(xnew_hour, ynew_avg_log_percent, color = '#4c78fb')\n",
    "\n",
    "        #Set the legends for the both graphs\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "        ax.legend(['ACTIVE_LOG_USED_PERCENT_MAX', 'ACTIVE_LOG_USED_PERCENT_AVG'], fontsize = 9, loc = 'upper center', bbox_to_anchor=(0.5,1.06), fancybox = True, shadow = True, ncol = 4)\n",
    "        \n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'LOG')):\n",
    "    df_predict = model_max.predict('2018-04-23 23', '2018-04-25 15', dynamic=True)\n",
    "    df_max[-1] = df_predict[0]\n",
    "    fig, ax = plt.subplots(figsize=(24, 7))\n",
    "    \n",
    "    x_data = df_max.loc['2018-04-22 08':].index\n",
    "    y_data = df_max.loc['2018-04-22 08':].values\n",
    "    \n",
    "    ax = df_max.loc['2018-04-22 08':].plot(ax=ax, label='ACTIVE_LOG_USED_PERCENT_MAX')\n",
    "    #To mark the the value for per point\n",
    "    for id in range(0,len(y_data),1):\n",
    "        pl.scatter(x_data[id], y_data[id],c = '#2c628b') \n",
    "    for id in range(0,len(y_data),2):\n",
    "        pl.text(x_data[id], y_data[id], '%.1f' % y_data[id], fontsize = 9)\n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    fig = model_max.plot_predict('2018-04-23 23', '2018-04-25 15', dynamic=True, ax=ax, plot_insample=False, alpha = 0.05)\n",
    "    figure_title = 'Max Log Usage Percentage by Hour\\n'\n",
    "    pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "    x_lable = 'Hours'\n",
    "    #To set x-axis label\n",
    "    pl.xlabel(x_lable)\n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'LOG')):\n",
    "    df_predict = model_avg.predict('2018-04-23 23', '2018-04-25 15', dynamic=True)\n",
    "    df_avg[-1] = df_predict[0]\n",
    "    fig, ax = plt.subplots(figsize=(24, 7))\n",
    "    \n",
    "    x_data = df_avg.loc['2018-04-22 08':].index\n",
    "    y_data = df_avg.loc['2018-04-22 08':].values\n",
    "    \n",
    "    ax = df_avg.loc['2018-04-22 08':].plot(ax=ax, label='ACTIVE_LOG_USED_PERCENT_AVG')\n",
    "    \n",
    "    #To mark the the value for per point\n",
    "    for id in range(0,len(y_data),1):\n",
    "        pl.scatter(x_data[id], y_data[id],c = '#2c628b') \n",
    "    for id in range(0,len(y_data),2):\n",
    "        pl.text(x_data[id], y_data[id], '%.1f' % y_data[id], fontsize = 9)\n",
    "        \n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    fig = model_avg.plot_predict('2018-04-23 23', '2018-04-25 15', dynamic=True, ax=ax, plot_insample=False, alpha = 0.05)\n",
    "    figure_title = 'Average Log Usage Percentage by Hour\\n'\n",
    "    pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    x_lable = 'Hours'\n",
    "    #To set x-axis label\n",
    "    pl.xlabel(x_lable)\n",
    "    pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
