{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the required libs\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import commonUtil\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "\n",
    "from scipy.interpolate import spline\n",
    "from sklearn import linear_model\n",
    "\n",
    "#To get all query conditions into a dictionary\n",
    "queryCondition = commonUtil.handle_query_condition()\n",
    "ISOK_ALL_PARA = queryCondition['ISOK_ALL_PARA']\n",
    "REPORT_TYPE = queryCondition['REPORT_TYPE']\n",
    "\n",
    "DB_SET = queryCondition['DB_SET']\n",
    "START_TIME = queryCondition['START_TIME']\n",
    "END_TIME = queryCondition['END_TIME']\n",
    "INTERVAL = queryCondition['INTERVAL']\n",
    "GENERAL_ERROR = queryCondition['GENERAL_ERROR']\n",
    "DB_CONN_ID = DB_SET[0]\n",
    "#To print the error of query conditon, only print once\n",
    "for err_id in range(len(queryCondition['GENERAL_ERROR'])):\n",
    "    print queryCondition['GENERAL_ERROR'][err_id]               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "According to original dataframe we can get a serial data set \n",
    "and use it to form a new dataframe.\n",
    "'''\n",
    "def get_train_df(dataframe_hour):\n",
    "    try:\n",
    "        #Get hour from original dataframe\n",
    "        x_hour_list = list(dataframe_hour['HOURS'].values)\n",
    "        #Get date from original dataframe\n",
    "        date_ori_list = list(dataframe_hour['DATE'].values)\n",
    "        #Get y_max_log_percent from original dataframe\n",
    "        y_max_log_percent = list(dataframe_hour['ACTIVE_LOG_USED_PERCENT_MAX'].values)\n",
    "        #Get y_avg_log_percent from original dataframe\n",
    "        y_avg_log_percent = list(dataframe_hour['ACTIVE_LOG_USED_PERCENT_AVG'].values)\n",
    "        \n",
    "        ##Get timestamp using hour and date from original dataframe\n",
    "        ts_list = []\n",
    "        for i in range(len(x_hour_list)):\n",
    "            hr_str = str(x_hour_list[i])\n",
    "            if len(hr_str) == 1:\n",
    "                hr_str = '0' + hr_str + ':00:00'\n",
    "            else:\n",
    "                hr_str = hr_str + ':00:00'\n",
    "            dt_str = str(date_ori_list[i]) + ' ' + hr_str\n",
    "            time_tuple = time.strptime(dt_str, '%Y-%m-%d %H:00:00')\n",
    "            ts = int(time.mktime(time_tuple))\n",
    "            ts_list.append(ts)\n",
    "        '''\n",
    "        Judge whether data set is serial or not\n",
    "        '''  \n",
    "        index_list = []\n",
    "        tmp_index = []\n",
    "        for j in range(len(ts_list) - 1):\n",
    "            if j not in tmp_index:\n",
    "                tmp_index.append(j)\n",
    "            if (ts_list[j + 1] - ts_list[j]) == 3600:\n",
    "                tmp_index.append(j + 1)\n",
    "                if (j + 1) == len(ts_list) - 1:\n",
    "                    index_list.append(tmp_index)\n",
    "                    tmp_index = None\n",
    "            else:\n",
    "                index_list.append(tmp_index)\n",
    "                tmp_index = []\n",
    "                if (j + 1) == len(ts_list) - 1:\n",
    "                    tmp_index.append(j + 1)\n",
    "                    index_list.append(tmp_index)\n",
    "                    tmp_index = []\n",
    "        max_continuous_index = []\n",
    "        \n",
    "        #for l in range(len(index_list)):\n",
    "            #max_len = len(index_list[0])\n",
    "            #if max_len <= len(index_list[l]):\n",
    "                #max_len = len(index_list[l])\n",
    "                #max_continuous_index = index_list[l]\n",
    "        \n",
    "        #Get the newest continuous data set\n",
    "        from_end_len = len(index_list) - 1\n",
    "        while from_end_len >= 0:\n",
    "            if(len(index_list[from_end_len]) >= 100):\n",
    "                max_continuous_index = index_list[len(index_list) - 1]\n",
    "                break\n",
    "            else:\n",
    "                if from_end_len == 0:\n",
    "                    max_continuous_index = index_list[len(index_list) - 1]\n",
    "                from_end_len_ -= 1\n",
    "        x_hour_list = x_hour_list[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        date_ori_list = date_ori_list[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        y_max_log_percent = y_max_log_percent[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        y_avg_log_percent = y_avg_log_percent[max_continuous_index[0]:max_continuous_index[len(max_continuous_index) - 1] + 1]\n",
    "        \n",
    "        df = pd.DataFrame()#[date_ori_list, x_hour_list, y_max_log_percent, y_avg_log_percent], columns=['DATE', 'HOURS', 'ACTIVE_LOG_USED_PERCENT_MAX', 'ACTIVE_LOG_USED_PERCENT_AVG'])\n",
    "        df['DATE'] = pd.Series(date_ori_list)\n",
    "        df['HOURS'] = pd.Series(x_hour_list)\n",
    "        df['ACTIVE_LOG_USED_PERCENT_MAX'] = pd.Series(y_max_log_percent)\n",
    "        df['ACTIVE_LOG_USED_PERCENT_AVG'] = pd.Series(y_avg_log_percent)\n",
    "        return df\n",
    "    except Exception as ex:\n",
    "        print str(ex)\n",
    "#To print the error of query conditon, only print once\n",
    "for err_id in range(len(GENERAL_ERROR)):\n",
    "    print GENERAL_ERROR[err_id]\n",
    "    \n",
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'LOG')):\n",
    "    DB_CONN_ID = DB_SET[0]\n",
    "    #Get data from the target database to assemble a data frame that will be used for the following graph\n",
    "    dataframe_interval = %sql SELECT char(date(COLLECTED)) as DATE, \\\n",
    "                              hour(COLLECTED) as HOURS, \\\n",
    "                              decimal(round(avg(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_AVG_KB, \\\n",
    "                              decimal(round(max(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_MAX_KB, \\\n",
    "                              decimal(round(avg(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_AVG_KB, \\\n",
    "                              decimal(round(max(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_MAX_KB, \\\n",
    "                              decimal(round(avg(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_AVG, \\\n",
    "                              decimal(round(max(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_MAX, \\\n",
    "                              decimal(round(avg(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_AVG_KB, \\\n",
    "                              decimal(round(max(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_MAX_KB, \\\n",
    "                              decimal(round(avg(decimal(SEC_LOG_USED_TOP, 17, 2))/1024, 2), 17, 2) as SEC_LOG_USED_TOP_AVG_KB, \\\n",
    "                              decimal(round(max(decimal(SEC_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as SEC_LOG_USED_TOP_MAX_KB, \\\n",
    "                              avg(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_AVG, max(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_MAX \\\n",
    "                              from IBM_DSM_VIEWS.MON_GET_TRANSACTION_LOG \\\n",
    "                              where dbconn_id='{DB_CONN_ID}' and collected >= '{START_TIME}' and collected < '{END_TIME}' \\\n",
    "                              group by date(COLLECTED), hour(COLLECTED) \\\n",
    "                              order by DATE,HOURS\n",
    "    \n",
    "    if dataframe_interval.empty:\n",
    "        print 'For Log: The query result is empty, please check your query parameters.\\n'\n",
    "        if os.path.exists(\"log.csv\"):\n",
    "            !rm log.csv\n",
    "    else:\n",
    "        #To save log data for generating table\n",
    "        if os.path.exists(\"log.csv\"):\n",
    "            !rm log.csv\n",
    "        dataframe_interval.to_csv(\"log.csv\", index_label = \"INDEX\")\n",
    "        \n",
    "        dataframe_hour_train = %sql SELECT char(date(COLLECTED)) as DATE, \\\n",
    "                                    hour(COLLECTED) as HOURS, \\\n",
    "                                    decimal(round(avg(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_AVG_KB, \\\n",
    "                                    decimal(round(max(decimal(TOTAL_LOG_USED, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_USED_MAX_KB, \\\n",
    "                                    decimal(round(avg(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_AVG_KB, \\\n",
    "                                    decimal(round(max(decimal(TOTAL_LOG_AVAILABLE, 17, 2)) / 1024, 2), 17, 2) as TOTAL_LOG_AVAILABLE_MAX_KB, \\\n",
    "                                    decimal(round(avg(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_AVG, \\\n",
    "                                    decimal(round(max(PCENTACTIVELOGUSED), 2), 17, 2) as ACTIVE_LOG_USED_PERCENT_MAX, \\\n",
    "                                    decimal(round(avg(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_AVG_KB, \\\n",
    "                                    decimal(round(max(decimal(TOT_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as TOT_LOG_USED_TOP_MAX_KB, \\\n",
    "                                    decimal(round(avg(decimal(SEC_LOG_USED_TOP, 17, 2))/1024, 2), 17, 2) as SEC_LOG_USED_TOP_AVG_KB, \\\n",
    "                                    decimal(round(max(decimal(SEC_LOG_USED_TOP, 17, 2)) / 1024, 2), 17, 2) as SEC_LOG_USED_TOP_MAX_KB, \\\n",
    "                                    avg(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_AVG, \\\n",
    "                                    max(SEC_LOGS_ALLOCATED) as SEC_LOGS_ALLOCATED_MAX \\\n",
    "                                    from IBM_DSM_VIEWS.MON_GET_TRANSACTION_LOG \\\n",
    "                                    where dbconn_id='{DB_CONN_ID}' and collected < '{END_TIME}' \\\n",
    "                                    group by date(COLLECTED), hour(COLLECTED) \\\n",
    "                                    order by DATE,HOURS\n",
    "        \n",
    "        #Get continuous data to build a predict model\n",
    "        df_train = get_train_df(dataframe_hour_train)\n",
    "        \n",
    "        #Get the hour data as x-axis from the data frame and convert the data into a numpy array\n",
    "        x_hour_list_train = list(df_train['HOURS'].values)\n",
    "        date_ori_train = list(df_train['DATE'].values)\n",
    "        #Get the data as y-axis from the data frame and convert the data into a numpy array\n",
    "        y_max_log_percent_list_train = list(df_train['ACTIVE_LOG_USED_PERCENT_MAX'].values)\n",
    "        #Get the data as y-axis from the data frame and convert the data into a numpy array\n",
    "        y_avg_log_percent_list_train = list(df_train['ACTIVE_LOG_USED_PERCENT_AVG'].values)\n",
    "        \n",
    "        #Predict INTERVAL points\n",
    "        predict_count = float('%.1f' % INTERVAL)\n",
    "\n",
    "        ######################\n",
    "        ## create predictive model with time series algorithm ARIMA(Autoregressive–moving-average model)\n",
    "        ## For the source data are non-seasonal, ARIMA(p, d, q) instead of ARIMA(p, d, q, s) is used to build model\n",
    "        ## ARIMA(p, d, q): \n",
    "        ##     AR(p) represents the P order autoregressive process; \n",
    "        ##     MA(q) represents the moving average process of the Q order; \n",
    "        ##     d respresents the number of difference\n",
    "        ## Refer to notebook ARIMA-Analysis for how to determine number of p, d, q with detail\n",
    "        \n",
    "        ## Generate pd.Series object for Time Series analysis \n",
    "        df_max = pd.Series(y_max_log_percent_list_train)\n",
    "        df_avg = pd.Series(y_avg_log_percent_list_train)\n",
    "        \n",
    "        datetime_str_train = []\n",
    "        for id_1 in range(len(date_ori_train)):\n",
    "            tmp_hour = str(x_hour_list_train[id_1])\n",
    "            if(len(tmp_hour) == 1):\n",
    "                tmp_hour = '0' + tmp_hour\n",
    "            tmp_dt = str(date_ori_train[id_1]) + ' ' + tmp_hour\n",
    "            datetime_str_train.append(tmp_dt)\n",
    "        ## \n",
    "        df_max.index = pd.PeriodIndex(start = datetime_str_train[0], end = datetime_str_train[len(datetime_str_train) - 1])\n",
    "        df_avg.index = pd.PeriodIndex(start = datetime_str_train[0], end = datetime_str_train[len(datetime_str_train) - 1])\n",
    "        \n",
    "        ## Apply data to ARIMA algorithm to generate Time Series analysis model with forcast data\n",
    "        model_max = sm.tsa.ARIMA(df_max,(7, 0, 1)).fit()\n",
    "        model_avg = sm.tsa.ARIMA(df_avg,(7, 0, 1)).fit()\n",
    "        \n",
    "        ## Handling interval\n",
    "        ## interval<=10, shows number of real data and predictive data same to interval\n",
    "        ## interval>10, shows number of real data same to interval and 10 predictive data\n",
    "   \n",
    "        #Define a empty var x_ticks to store x-axis ticks(marks)\n",
    "        x_ticks = []\n",
    "        #Define a empty var x_ticks_lables to restore x-axis labels\n",
    "        x_ticks_lables = []\n",
    "        #Define a empty var y_ticks to store y-axis ticks(marks)\n",
    "        y_max_log_percent_list = list(dataframe_interval['ACTIVE_LOG_USED_PERCENT_MAX'].values)\n",
    "        y_avg_log_percent_list = list(dataframe_interval['ACTIVE_LOG_USED_PERCENT_AVG'].values)\n",
    "        \n",
    "        end_dt_str = END_TIME[0:13] + \":00:00\"\n",
    "        end_tuple = time.strptime(end_dt_str, '%Y-%m-%d %H:00:00')\n",
    "        end_timestamp = time.mktime(end_tuple)\n",
    "        \n",
    "        previous_hour = None\n",
    "        predict_hour = None\n",
    "        aft_hour_st = None\n",
    "        pre_hour_st = end_timestamp - 3600.0 * INTERVAL\n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(pre_hour_st)\n",
    "        previous_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "        \n",
    "        aft_hour_st = end_timestamp + 3600 * predict_count\n",
    "            \n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(aft_hour_st)\n",
    "        predict_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "        \n",
    "        ## Define format of x-Axis\n",
    "        date_all = []\n",
    "        x_hour = []\n",
    "        loop_count = int((aft_hour_st - pre_hour_st)/3600.0)\n",
    "        for id_2 in range(loop_count):\n",
    "            tmp_hour_st = pre_hour_st + id_2 * 3600.0\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(tmp_hour_st)\n",
    "            date_all.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:10])\n",
    "            x_hour.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[11:13])\n",
    "        commonUtil.format_x_axis(date_all, x_hour, x_ticks, x_ticks_lables)\n",
    "        ## Apply data to model to generate forcast data\n",
    "        predict_array_max = model_max.predict(datetime_str_train[len(datetime_str_train) - 1], predict_hour, dynamic=True)\n",
    "        #print 'predict_array_max=',predict_array_max\n",
    "        #predict_array_max_list = list(predict_array_max)\n",
    "        predict_array_max = predict_array_max[1:INTERVAL + 1]\n",
    "        ## Prepare data for Y-Axis\n",
    "        y_max_log_percent_list = y_max_log_percent_list[-INTERVAL:] + list(predict_array_max)\n",
    "        \n",
    "        ## Apply data to model to generate forcast data\n",
    "        predict_array_avg = model_avg.predict(datetime_str_train[len(datetime_str_train) - 1], predict_hour, dynamic=True)\n",
    "        predict_array_avg =  predict_array_max = predict_array_avg[1:INTERVAL + 1]\n",
    "        y_avg_log_percent_list = y_avg_log_percent_list[-INTERVAL:] + list(predict_array_avg)\n",
    "        \n",
    "        #ax = df_final.ix[previous_ten_hour:].plot(ax=ax)\n",
    "        #predict_sunspots.plot(ax=ax)\n",
    "        fig, ax = pl.subplots(figsize=(24, 7))\n",
    "        figure_title = 'Max and Average Log Usage Percentage by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        x_lable = 'Hours'\n",
    "        #To set x-axis label\n",
    "        pl.xlabel(x_lable)\n",
    "        #To set y-axis label\n",
    "        pl.ylabel(u'ACTIVE_LOG_USED_PERCENT %')\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        \n",
    "        ##To scatter the date of y_max_log_percent_list and mark the data point \n",
    "        for id in range(0,len(y_max_log_percent_list),2):\n",
    "            if(y_max_log_percent_list[id] == 0.0):#If no data,drawing a empty circle\n",
    "                if id >= len(y_max_log_percent_list) - predict_count and id < len(y_max_log_percent_list):\n",
    "                    #This is the data of prediction and empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = 'r') \n",
    "                else:\n",
    "                    #This is the real data of y_max_log_percent and empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "            else:#Data not empty\n",
    "                if id >= len(y_max_log_percent_list) - predict_count and id < len(y_max_log_percent_list):\n",
    "                    #This is the data of prediction and not empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = 'r') \n",
    "                    pl.text(x_ticks[id], y_max_log_percent_list[id], '%.1f' % y_max_log_percent_list[id], fontsize = 9)\n",
    "                else:\n",
    "                    #This is the real data of y_max_log_percent and not empty\n",
    "                    pl.scatter(x_ticks[id], y_max_log_percent_list[id], c = '#60bdae') \n",
    "                    pl.text(x_ticks[id], y_max_log_percent_list[id], '%.1f' % y_max_log_percent_list[id], fontsize = 9)\n",
    "        \n",
    "        #### smooth the lines\n",
    "        for id in range(1, len(y_avg_log_percent_list), 2):\n",
    "            if(y_avg_log_percent_list[id] == 0.0):#If no data,drawing a empty circle\n",
    "                if id >= len(y_avg_log_percent_list) - predict_count and id < len(y_avg_log_percent_list):\n",
    "                    #This is the data of prediction and empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = 'r') \n",
    "                else:\n",
    "                    #This is the real data of y_avg_log_percent and empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "            else:#Data not empty\n",
    "                if id >= len(y_avg_log_percent_list) - predict_count and id < len(y_avg_log_percent_list):\n",
    "                    #This is the data of prediction and not empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = 'r') \n",
    "                    pl.text(x_ticks[id], y_avg_log_percent_list[id], '%.1f' % y_avg_log_percent_list[id], fontsize = 9)\n",
    "                else:\n",
    "                    #This is the real data of y_avg_log_percent and not empty\n",
    "                    pl.scatter(x_ticks[id], y_avg_log_percent_list[id], c = '#60bdae') \n",
    "                    pl.text(x_ticks[id], y_avg_log_percent_list[id], '%.1f' % y_avg_log_percent_list[id], fontsize = 9)\n",
    "        \n",
    "        xnew_hour = []\n",
    "        ynew_max_log_percent = []\n",
    "        ynew_avg_log_percent = []\n",
    "        \n",
    "        ##In order to smooth the line chart,handle the data further##\n",
    "        #Expand each x axis data 20 times\n",
    "        xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "        #Handle the data of new y axis data\n",
    "        ynew_max_log_percent = spline(np.asarray(x_ticks), np.asarray(y_max_log_percent_list), xnew_hour)\n",
    "        ynew_max_log_percent_list = list(ynew_max_log_percent)\n",
    "        \n",
    "        ynew_avg_log_percent = spline(np.asarray(x_ticks), np.asarray(y_avg_log_percent_list), xnew_hour)\n",
    "        ynew_avg_log_percent_list = list(ynew_avg_log_percent)\n",
    "        #No negative value for y-axis\n",
    "        for y_idx in range(len(ynew_max_log_percent_list)):\n",
    "            if (ynew_max_log_percent_list[y_idx] < 0.0):\n",
    "                ynew_max_log_percent_list[y_idx] = 0.0\n",
    "        ynew_max_log_percent = np.asarray(ynew_max_log_percent_list)\n",
    "        \n",
    "        #No negative value for y-axis\n",
    "        for y_idx in range(len(ynew_avg_log_percent_list)):\n",
    "            if (ynew_avg_log_percent_list[y_idx] < 0.0):\n",
    "                ynew_avg_log_percent_list[y_idx] = 0.0\n",
    "        ynew_avg_log_percent = np.asarray(ynew_avg_log_percent_list)\n",
    "\n",
    "        #Fill the gragh according to your requirement  tian chong yin ying\n",
    "        max_ratio = len(list(predict_array_max)) / float('%.1f' % len(y_max_log_percent_list))\n",
    "        pl.fill_between(xnew_hour, ynew_max_log_percent, where=(xnew_hour.min()<xnew_hour) & (xnew_hour<xnew_hour.max() * (1.0 - max_ratio)), color = '#60bdae', alpha = 0.15)\n",
    "        pl.fill_between(xnew_hour, ynew_max_log_percent, where=(xnew_hour.max() * (1.0 - max_ratio) <xnew_hour) & (xnew_hour<xnew_hour.max()), color = '#60bdae', alpha = 0.5)\n",
    "        #Draw curve graph\n",
    "        pl.plot(xnew_hour, ynew_max_log_percent, color = '#60bdae')\n",
    "        \n",
    "        #Fill the gragh according to your requirement\n",
    "        avg_ratio = len(list(predict_array_avg)) / float('%.1f' % len(y_avg_log_percent_list))\n",
    "        pl.fill_between(xnew_hour, ynew_avg_log_percent, where=(xnew_hour.min() < xnew_hour) & (xnew_hour < xnew_hour.max() * (1.0 - avg_ratio)), color = '#4c78fb', alpha = 0.15)\n",
    "        pl.fill_between(xnew_hour, ynew_avg_log_percent, where=(xnew_hour.max() * (1.0 - avg_ratio) < xnew_hour) & (xnew_hour < xnew_hour.max()), color = '#4c78fb', alpha = 0.5)\n",
    "        #Draw curve graph\n",
    "        pl.plot(xnew_hour, ynew_avg_log_percent, color = '#4c78fb')\n",
    "\n",
    "        #Set the legends for the both graphs\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "        ax.legend(['ACTIVE_LOG_USED_PERCENT_MAX', 'ACTIVE_LOG_USED_PERCENT_AVG'], fontsize = 9, loc = 'upper center', bbox_to_anchor=(0.5,1.06), fancybox = True, shadow = True, ncol = 4)\n",
    "        \n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'LOG')):\n",
    "    df_predict = model_max.predict('2018-04-23 23', '2018-04-25 15', dynamic=True)\n",
    "    df_max[-1] = df_predict[0]\n",
    "    fig, ax = plt.subplots(figsize=(24, 7))\n",
    "    \n",
    "    x_data = df_max.loc['2018-04-22 08':].index\n",
    "    y_data = df_max.loc['2018-04-22 08':].values\n",
    "    \n",
    "    ax = df_max.loc['2018-04-22 08':].plot(ax=ax, label='ACTIVE_LOG_USED_PERCENT_MAX')\n",
    "    #To mark the the value for per point\n",
    "    for id in range(0,len(y_data),1):\n",
    "        pl.scatter(x_data[id], y_data[id],c = '#2c628b') \n",
    "    for id in range(0,len(y_data),2):\n",
    "        pl.text(x_data[id], y_data[id], '%.1f' % y_data[id], fontsize = 9)\n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    fig = model_max.plot_predict('2018-04-23 23', '2018-04-25 15', dynamic=True, ax=ax, plot_insample=False, alpha = 0.05)\n",
    "    figure_title = 'Max Log Usage Percentage by Hour\\n'\n",
    "    pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "    x_lable = 'Hours'\n",
    "    #To set x-axis label\n",
    "    pl.xlabel(x_lable)\n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'LOG')):\n",
    "    df_predict = model_avg.predict('2018-04-23 23', '2018-04-25 15', dynamic=True)\n",
    "    df_avg[-1] = df_predict[0]\n",
    "    fig, ax = plt.subplots(figsize=(24, 7))\n",
    "    \n",
    "    x_data = df_avg.loc['2018-04-22 08':].index\n",
    "    y_data = df_avg.loc['2018-04-22 08':].values\n",
    "    \n",
    "    ax = df_avg.loc['2018-04-22 08':].plot(ax=ax, label='ACTIVE_LOG_USED_PERCENT_AVG')\n",
    "    \n",
    "    #To mark the the value for per point\n",
    "    for id in range(0,len(y_data),1):\n",
    "        pl.scatter(x_data[id], y_data[id],c = '#2c628b') \n",
    "    for id in range(0,len(y_data),2):\n",
    "        pl.text(x_data[id], y_data[id], '%.1f' % y_data[id], fontsize = 9)\n",
    "        \n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    fig = model_avg.plot_predict('2018-04-23 23', '2018-04-25 15', dynamic=True, ax=ax, plot_insample=False, alpha = 0.05)\n",
    "    figure_title = 'Average Log Usage Percentage by Hour\\n'\n",
    "    pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "    pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "    x_lable = 'Hours'\n",
    "    #To set x-axis label\n",
    "    pl.xlabel(x_lable)\n",
    "    pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
