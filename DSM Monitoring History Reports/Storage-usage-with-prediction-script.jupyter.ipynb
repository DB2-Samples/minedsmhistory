{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the required libs\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import commonUtil\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy.interpolate import spline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#To get all query conditions into a dictionary\n",
    "queryCondition = commonUtil.handle_query_condition()\n",
    "ISOK_ALL_PARA = queryCondition['ISOK_ALL_PARA']\n",
    "REPORT_TYPE = queryCondition['REPORT_TYPE']\n",
    "\n",
    "DB_SET = queryCondition['DB_SET']\n",
    "START_TIME = queryCondition['START_TIME']\n",
    "END_TIME = queryCondition['END_TIME']\n",
    "INTERVAL = queryCondition['INTERVAL']\n",
    "TBSP_NAME = queryCondition['TBSP_NAME']\n",
    "\n",
    "#To print the error of query conditon, only print once\n",
    "for err_id in range(len(queryCondition['GENERAL_ERROR'])):\n",
    "    print queryCondition['GENERAL_ERROR'][err_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'TBSP_TABLE')):\n",
    "    DB_CONN_ID = DB_SET[0] \n",
    "    #To get data from the target database to assemble a data frame that will be used for the following graph\n",
    "    df_interval = %sql with temp as( select date(IBMIOCM_TIMESTAMP) as date, \\\n",
    "                       hour(IBMIOCM_TIMESTAMP) as hour, \\\n",
    "                       tbsp_name, \\\n",
    "                       (decimal(tbsp_total_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, \\\n",
    "                       (decimal(tbsp_used_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 / 1024 as TBSP_USED_SIZE_MB, \\\n",
    "                       (decimal(tbsp_free_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 / 1024 as TBSP_FREE_SIZE_MB, \\\n",
    "                       (decimal(tbsp_usable_pages,17,2) * decimal(tbsp_page_size,17,2)) / 1024 /1024 as TBSP_USABLE_SIZE_MB, \\\n",
    "                       case when tbsp_usable_pages > 0 then decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 \\\n",
    "                       when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT \\\n",
    "                       from IBMIOCM.DB2LUW_MONTABLESPACE_HIS \\\n",
    "                       where IBMIOCM_DATABASE = '{DB_CONN_ID}' and IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}' \\\n",
    "                       order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)), temp_stg \\\n",
    "                       as ( select row_number()over(partition by date, hour \\\n",
    "                       order by TBSP_USED_SIZE_MB desc) as \\\n",
    "                       row_id, date, hour, tbsp_name, TBSP_TOTAL_SIZE_MB, TBSP_USED_SIZE_MB, TBSP_FREE_SIZE_MB, TBSP_USABLE_SIZE_MB, TBSP_UTILIZATION_PERCENT \\\n",
    "                       from temp) \\\n",
    "                       select date, hour, tbsp_name, TBSP_TOTAL_SIZE_MB, TBSP_USED_SIZE_MB, TBSP_FREE_SIZE_MB, TBSP_USABLE_SIZE_MB, TBSP_UTILIZATION_PERCENT, row_id as rank \\\n",
    "                       from temp_stg where row_id <= 3; \n",
    "            \n",
    "    if os.path.exists(\"tbsp.csv\"):\n",
    "        !rm tbsp.csv\n",
    "    if df_interval.empty:\n",
    "        print 'For Tablespace: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To judge whether file exists or not and if yes, delete it, if not save the data into a file storage.csv\n",
    "        df_interval.to_csv(\"tbsp.csv\", index_label = \"INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'TABLESPACE')):\n",
    "    DB_CONN_ID = DB_SET[0]\n",
    "    #dataframe_week = %sql select year(collected) as year, week(collected) as week, substr(dbconn_id,1,30) as conn_name, date(min(collected)) as begin_date, date(max(collected)) as end_date, sum(total_cpu_usec_delta) / 1000000.0 as cpu_sec, sum(logical_reads_delta) as logical_reads, sum(physical_reads_delta) as physical_reads, sum(act_completed_total_delta) as activities, sum(total_app_commits_delta) as commits, sum(total_act_time_delta) / (sum(act_aborted_total_delta) + sum(act_completed_total_delta)) as avg_activity_time_msec from ibm_dsm_views.throughput_all group by year(collected), week(collected), dbconn_id order by year(collected), week(collected), dbconn_id \n",
    "    df_interval = %sql select date(IBMIOCM_TIMESTAMP) as date, \\\n",
    "                       hour(IBMIOCM_TIMESTAMP) as hour, \\\n",
    "                       tbsp_name, (tbsp_total_pages * tbsp_page_size) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, \\\n",
    "                       (tbsp_used_pages * tbsp_page_size) / 1024 / 1024 as TBSP_USED_SIZE_MB, \\\n",
    "                       (tbsp_free_pages * tbsp_page_size) / 1024 / 1024 as TBSP_FREE_SIZE_MB, \\\n",
    "                       (tbsp_usable_pages * tbsp_page_size) / 1024 /1024 as TBSP_USABLE_SIZE_MB, \\\n",
    "                       case when tbsp_usable_pages > 0 then \\\n",
    "                       decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 \\\n",
    "                       when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT \\\n",
    "                       from IBMIOCM.DB2LUW_MONTABLESPACE_HIS \\\n",
    "                       where IBMIOCM_DATABASE='{DB_CONN_ID}' and tbsp_name = '{TBSP_NAME}' and IBMIOCM_TIMESTAMP >= '{START_TIME}' and IBMIOCM_TIMESTAMP < '{END_TIME}' \\\n",
    "                       order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)\n",
    "    if os.path.exists(\"tablespace.csv\"):\n",
    "        !rm tablespace.csv\n",
    "    if df_interval.empty:\n",
    "        print 'For Tablespace Table: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To save tablespace data for generating table\n",
    "        df_interval.to_csv(\"tablespace.csv\", index_label = \"INDEX\")\n",
    "        dataframe_tbsp = %sql select date(IBMIOCM_TIMESTAMP) as date, \\\n",
    "                              hour(IBMIOCM_TIMESTAMP) as hour, \\\n",
    "                              tbsp_name, \\\n",
    "                              (tbsp_total_pages * tbsp_page_size) / 1024 /1024 as TBSP_TOTAL_SIZE_MB, \\\n",
    "                              (tbsp_used_pages * tbsp_page_size) / 1024 / 1024 as TBSP_USED_SIZE_MB, \\\n",
    "                              (tbsp_free_pages * tbsp_page_size) / 1024 / 1024 as TBSP_FREE_SIZE_MB, \\\n",
    "                              (tbsp_usable_pages * tbsp_page_size) / 1024 /1024 as TBSP_USABLE_SIZE_MB, \\\n",
    "                              case when tbsp_usable_pages > 0 then decimal(round((decimal(tbsp_used_pages,17,2) / decimal(tbsp_usable_pages,17,2)),2),17,2) * 100 \\\n",
    "                              when tbsp_usable_pages = 0 then 0 else NULL end as TBSP_UTILIZATION_PERCENT \\\n",
    "                              from IBMIOCM.DB2LUW_MONTABLESPACE_HIS \\\n",
    "                              where IBMIOCM_DATABASE='{DB_CONN_ID}' and tbsp_name = '{TBSP_NAME}' and IBMIOCM_TIMESTAMP < '{END_TIME}'\\\n",
    "                              order by date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP)\n",
    "        \n",
    "        x_hour = dataframe_tbsp['HOUR'].values\n",
    "        x_hour_list = list(x_hour)\n",
    "        y_tbsp_util_percent = dataframe_tbsp['TBSP_UTILIZATION_PERCENT'].values\n",
    "        y_tbsp_util_percent_list = list(y_tbsp_util_percent)\n",
    "        date_all = list(dataframe_tbsp['DATE'].values)\n",
    "        \n",
    "        ori_datetime_str = commonUtil.get_original_datatime_str(date_all, x_hour_list)\n",
    "        #To set predict_count according to INTERVAL and max value is 10\n",
    "        if(INTERVAL <= 10):\n",
    "            predict_count = float('%.1f' % INTERVAL)\n",
    "        else:\n",
    "            predict_count = 10.0\n",
    "\n",
    "        #To form datetime_str from date_ori for indexing\n",
    "        datetime_str = []\n",
    "        for id_1 in range(len(date_all)):\n",
    "            tmp_hour = str(x_hour_list[id_1])\n",
    "            if(len(tmp_hour) == 1):\n",
    "                tmp_hour = '0' + tmp_hour\n",
    "            tmp_dt = str(date_all[id_1]) + ' ' + tmp_hour\n",
    "            datetime_str.append(tmp_dt)\n",
    "            \n",
    "        '''\n",
    "        To complement data for no data some hours\n",
    "        '''\n",
    "        #To get start timestamp First \n",
    "        min_dt_str = datetime_str[0] + ':00:00'\n",
    "        min_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "        min_timestamp = time.mktime(min_tuple)\n",
    "            \n",
    "        #To get end timestamp \n",
    "        max_tuple = time.strptime(END_TIME[0:13] + \":00:00\", '%Y-%m-%d %H:00:00')\n",
    "        max_timestamp = time.mktime(max_tuple)\n",
    "\n",
    "        hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "        #clear date_all\n",
    "        date_all = []\n",
    "        #clear x_hour_list\n",
    "        x_hour_list = []\n",
    "        ori_dt_str = []\n",
    "        all_tbsp_util_list_1 = []\n",
    "\n",
    "        #Reassign 4 variables above\n",
    "        for id_2 in range(hour_diff):\n",
    "            tmp_st = min_timestamp + id_2 * 3600.0\n",
    "            #To change timestamp to datetime string\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "            tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "            tmp_date_str = tmp_datetime_str[0:10]\n",
    "            tmp_hour_str = tmp_datetime_str[11:13]\n",
    "            ori_dt_str.append(tmp_date_str + ' ' + tmp_hour_str)\n",
    "            date_all.append(tmp_date_str)\n",
    "            x_hour_list.append(tmp_hour_str)\n",
    "\n",
    "            '''\n",
    "            Below code is for handling y-axis's data\n",
    "            If there is no data at this hour,0.0 will be filled into\n",
    "            '''\n",
    "            if tmp_datetime_str in ori_datetime_str:\n",
    "                tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                all_tbsp_util_list_1.append(y_tbsp_util_percent_list[tmp_index])\n",
    "            else:\n",
    "                all_tbsp_util_list_1.append(1.0)\n",
    "            \n",
    "        df_final = pd.Series(all_tbsp_util_list_1)\n",
    "        df_final.index = pd.PeriodIndex(start = ori_dt_str[0], end = ori_dt_str[len(ori_dt_str) - 1])\n",
    "        tbsp_model = sm.tsa.ARIMA(df_final,(7,0,1)).fit()\n",
    "            \n",
    "        #To get end_timestamp from ori_dt_str for preparing to form x_ticks and x_ticks_lables\n",
    "        end_dt_str = ori_dt_str[len(ori_dt_str) - 1] + \":00:00\"\n",
    "        end_tuple = time.strptime(end_dt_str, '%Y-%m-%d %H:00:00')\n",
    "        end_timestamp = time.mktime(end_tuple)\n",
    "            \n",
    "        #The count of output original data \n",
    "        ori_hour = None\n",
    "        ori_hour_st = end_timestamp - 3600.0 * (INTERVAL - 1)\n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(ori_hour_st)\n",
    "        ori_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "\n",
    "        #The count of predict data \n",
    "        predict_hour = None\n",
    "        predict_hour_st = None\n",
    "        #If INTERVAL>10，we will predict only 10 hours\n",
    "        #If not we we will predict INTERVAL hours\n",
    "        if(INTERVAL <= 10):\n",
    "            predict_hour_st = end_timestamp + 3600.0 * INTERVAL\n",
    "        else:\n",
    "            predict_hour_st = end_timestamp + 3600 * predict_count\n",
    "        tmp_datetime = datetime.datetime.fromtimestamp(predict_hour_st)\n",
    "        predict_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "            \n",
    "        #To get predict data\n",
    "        predict_array = tbsp_model.predict(ori_dt_str[len(ori_dt_str) - 1], predict_hour, dynamic=True)\n",
    "        predict_array_list = list(predict_array)\n",
    "        predict_array_list[0] = all_tbsp_util_list_1[len(predict_array_list) - 1]\n",
    "        predict_array = np.asarray(predict_array_list[1:len(predict_array_list)])\n",
    "        all_tbsp_util_list_1 = all_tbsp_util_list_1[-INTERVAL:] + list(predict_array)\n",
    "           \n",
    "        #To set x ticks and x label\n",
    "        loop_count = int((predict_hour_st - ori_hour_st)/3600.0)\n",
    "        date_all = []\n",
    "        x_hour = []\n",
    "        for id_3 in range(loop_count + 1):\n",
    "            tmp_hour_st = ori_hour_st + id_3 * 3600.0\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(tmp_hour_st)\n",
    "            date_all.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:10])\n",
    "            x_hour.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[11:13])\n",
    "\n",
    "        x_ticks = []\n",
    "        x_ticks_lables = []\n",
    "        if len(x_ticks) == 0:\n",
    "            commonUtil.format_x_axis(date_all, x_hour, x_ticks, x_ticks_lables)\n",
    "\n",
    "        #To declare a Sketchpad\n",
    "        fig = pl.figure()\n",
    "        #To declare an ax container as a drawing paper\n",
    "        ax_tbsp_util= fig.add_subplot(111)\n",
    "\n",
    "        data_size = len(x_ticks)\n",
    "        if(data_size <= 20):\n",
    "            fig.set_size_inches(12,6)\n",
    "        elif(data_size <= 40):\n",
    "            fig.set_size_inches(16,6)\n",
    "        elif(data_size <= 60):\n",
    "            fig.set_size_inches(18,7)\n",
    "        elif(data_size <=100):\n",
    "            fig.set_size_inches(22,7)\n",
    "\n",
    "        #To set the title/label/grid for the graph\n",
    "        figure_title = 'The Table Space Utilization Percent by Hour\\n'\n",
    "        pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "        x_lable = 'Hours'\n",
    "        #To set x-axis label\n",
    "        pl.xlabel(x_lable)\n",
    "        #To set y-axis label\n",
    "        pl.ylabel(u'TBSP_UTILIZATION_PERCENT %')\n",
    "        #To set grid line style according to your requirement\n",
    "        pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "        pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        \n",
    "        #To scatter the data of y-axis and mark the data point\n",
    "        for id_5 in range(len(all_tbsp_util_list_1)):\n",
    "                if(all_tbsp_util_list_1[id_5] == 0.0):#If no data,drawing a empty circle\n",
    "                    if id_5 >= len(all_tbsp_util_list_1) - predict_count and id_5 < len(all_tbsp_util_list_1):\n",
    "                        #This is the data of prediction and empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = 'r') \n",
    "                    else:\n",
    "                        #This is the real data of y_max_log_percent and empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                else:#Data not empty\n",
    "                    if id_5 >= len(all_tbsp_util_list_1) - predict_count and id_5 < len(all_tbsp_util_list_1):\n",
    "                        #This is the data of prediction and not empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = 'r') \n",
    "                        pl.text(x_ticks[id_5], all_tbsp_util_list_1[id_5], '%.0f' % all_tbsp_util_list_1[id_5], fontsize = 9)\n",
    "                    else:\n",
    "                        #This is the real data of y_max_log_percent and not empty\n",
    "                        pl.scatter(x_ticks[id_5], all_tbsp_util_list_1[id_5], c = '#4c78fb') \n",
    "                        pl.text(x_ticks[id_5], all_tbsp_util_list_1[id_5], '%.0f' % all_tbsp_util_list_1[id_5], fontsize = 9)\n",
    "\n",
    "        #Below two variable is used for storeing magnified data\n",
    "        xnew_hour = []\n",
    "        ynew_tbsp_util_percent = []\n",
    "        #Expand each x axis data 20 times\n",
    "        xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "        #Handle the data of new y axis data\n",
    "        ynew_tbsp_util_percent = spline(np.asarray(x_ticks), np.asarray(all_tbsp_util_list_1), xnew_hour)\n",
    "        ynew_tbsp_util_percent_list = list(ynew_tbsp_util_percent)\n",
    "        #No negative value for y-axis\n",
    "        for y_idx in range(len(ynew_tbsp_util_percent_list)):\n",
    "            if (ynew_tbsp_util_percent_list[y_idx] < 0.0):\n",
    "                ynew_tbsp_util_percent_list[y_idx] = 0.0\n",
    "        ynew_tbsp_util_percent = np.asarray(ynew_tbsp_util_percent_list)\n",
    "\n",
    "        #Fill the gragh according to your requirement\n",
    "        tbsp_ratio = len(list(predict_array)) / float('%.1f' % len(all_tbsp_util_list_1))\n",
    "        #pl.fill_between(xnew_hour, ynew_tbsp_util_percent, where=(xnew_hour.min() < xnew_hour) & (xnew_hour < xnew_hour.max() * (1.0 - tbsp_ratio)), color = '#4c78fb', alpha = 0.15)\n",
    "        pl.fill_between(xnew_hour, ynew_tbsp_util_percent, where=(xnew_hour.max() * (1.0 - tbsp_ratio) < xnew_hour) & (xnew_hour < xnew_hour.max()), color = '#2c628b', alpha = 0.1)\n",
    "        #Draw curve graph\n",
    "        pl.plot(xnew_hour, ynew_tbsp_util_percent, color = '#2c628b')\n",
    "        pl.yticks(np.arange(0, 101, 10))\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#To get data and draw graph by data.\n",
    "if(ISOK_ALL_PARA and (REPORT_TYPE == 'ALL' or REPORT_TYPE == 'DATABASE')):\n",
    "    #To store all table spaces' usage information for drawing graph\n",
    "    all_db_util_list = []\n",
    "    #x ticks sorted by asc\n",
    "    x_ticks = None\n",
    "    #real x ticks label\n",
    "    x_ticks_labels = None\n",
    "    #per legend related with each table space\n",
    "    legend_title_db = []\n",
    "    #To get data from the target table to assemble a data frame that will be used for table view\n",
    "    #df_interval = %sql with temp as (select ibmiocm_database as dbname, IBMIOCM_TIMESTAMP, decimal(round(decimal((0.000000954*SUM(TBSP_USED_PAGES * TBSP_PAGE_SIZE)),17,2),2), 17,2) AS USAGE_GB FROM IBMIOCM.DATABASE_HIS where IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}'  group by ibmiocm_database, IBMIOCM_TIMESTAMP ORDER BY ibmiocm_database, IBMIOCM_TIMESTAMP), temp_stg as(select dbname as dbname, date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, max(USAGE_GB) as MAX_USAGE_GB from temp group by dbname, date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP) ORDER BY dbname, date, hour), temp_stg2 as(select row_number() over (partition by date order by MAX_USAGE_GB desc) as row_id, dbname, date, hour, MAX_USAGE_GB from temp_stg) select dbname, date, hour, MAX_USAGE_GB, row_id as rank from temp_stg2 where row_id <= 5\n",
    "    df_interval = %sql with temp as (select ibmiocm_database as dbname, \\\n",
    "                       IBMIOCM_TIMESTAMP, \\\n",
    "                       decimal(round(decimal((0.000000954*SUM(TBSP_USED_PAGES * TBSP_PAGE_SIZE)),17,2),2), 17,2) AS USAGE_GB \\\n",
    "                       FROM IBMIOCM.DATABASE_HIS \\\n",
    "                       where IBMIOCM_TIMESTAMP >= '{START_TIME}' and  IBMIOCM_TIMESTAMP < '{END_TIME}'  \\\n",
    "                       group by ibmiocm_database, IBMIOCM_TIMESTAMP \\\n",
    "                       ORDER BY ibmiocm_database, IBMIOCM_TIMESTAMP), \\\n",
    "                       temp_stg as (\\\n",
    "                       select dbname as dbname, \\\n",
    "                       date(IBMIOCM_TIMESTAMP) as date, \\\n",
    "                       hour(IBMIOCM_TIMESTAMP) as hour, \\\n",
    "                       max(USAGE_GB) as MAX_USAGE_GB \\\n",
    "                       from temp \\\n",
    "                       group by dbname, date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP) \\\n",
    "                       ORDER BY dbname, date, hour), temp_stg2 as(select row_number() over \\\n",
    "                       (partition by date,hour order by MAX_USAGE_GB desc) as row_id, \\\n",
    "                       dbname, date, hour, MAX_USAGE_GB from temp_stg) \\\n",
    "                       select dbname, date, hour, MAX_USAGE_GB, row_id as rank \\\n",
    "                       from temp_stg2 \\\n",
    "                       where row_id <= 5\n",
    "                \n",
    "    if os.path.exists(\"db.csv\"):\n",
    "        !rm db.csv\n",
    "    if df_interval.empty:\n",
    "        print 'For databases: The query result is empty, please check your query parameters.\\n'\n",
    "    else:\n",
    "        #To get top5 database\n",
    "        db_name_list = list(df_interval['DBNAME'].values)\n",
    "        db_name_set = set(db_name_list)\n",
    "        db_name_list = list(db_name_set)\n",
    "        #To judge whether file exists or not and if yes, delete it, if not save the data into a file db.csv\n",
    "        df_interval.to_csv(\"db.csv\", index_label = \"INDEX\")\n",
    "        for id_0 in range(len(db_name_list)):\n",
    "            tmp_db_name_str = str(db_name_list[id_0])\n",
    "            legend_title_db.append(tmp_db_name_str)\n",
    "            df_db_hour = %sql with temp as (select ibmiocm_database as dbname, IBMIOCM_TIMESTAMP, decimal(round(decimal((0.000000954*SUM(TBSP_USED_PAGES * TBSP_PAGE_SIZE)),17,2),2), 17,2) AS USAGE_GB FROM IBMIOCM.DATABASE_HIS where ibmiocm_database = '{tmp_db_name_str}' and IBMIOCM_TIMESTAMP < '{END_TIME}' group by ibmiocm_database, IBMIOCM_TIMESTAMP ORDER BY ibmiocm_database, IBMIOCM_TIMESTAMP), temp_stg as(select dbname as dbname, date(IBMIOCM_TIMESTAMP) as date, hour(IBMIOCM_TIMESTAMP) as hour, max(USAGE_GB) as MAX_USAGE_GB from temp group by dbname, date(IBMIOCM_TIMESTAMP), hour(IBMIOCM_TIMESTAMP) ORDER BY dbname, date, hour), temp_stg2 as(select row_number() over (partition by date order by MAX_USAGE_GB desc) as row_id, dbname, date, hour, MAX_USAGE_GB from temp_stg) select dbname, date, hour, MAX_USAGE_GB, row_id as rank from temp_stg2\n",
    "            #To get the hour data as x-axis from the data frame and convert the data into a numpy array\n",
    "            x_hour = df_db_hour['HOUR'].values\n",
    "            x_hour_list = list(x_hour)\n",
    "            #To get the data as y-axis from the data frame and convert the data into a numpy array\n",
    "            y_db_util_percent = df_db_hour['MAX_USAGE_GB'].values\n",
    "            #To chenage y_tbsp_util_percent into y_db_util_percent_list\n",
    "            y_db_util_percent_list = list(y_db_util_percent)\n",
    "            \n",
    "            date_all = list(df_db_hour['DATE'].values)\n",
    "            \n",
    "            ori_datetime_str = commonUtil.get_original_datatime_str(date_all, x_hour_list)\n",
    "            \n",
    "            #To set predict_count according to INTERVAL and max value is 10\n",
    "            if(INTERVAL <= 10):\n",
    "                predict_count = float('%.1f' % INTERVAL)\n",
    "            else:\n",
    "                predict_count = 10.0\n",
    "            \n",
    "            #To form datetime_str from date_ori for indexing\n",
    "            datetime_str = []\n",
    "            for id_1 in range(len(date_all)):\n",
    "                tmp_hour = str(x_hour_list[id_1])\n",
    "                if(len(tmp_hour) == 1):\n",
    "                    tmp_hour = '0' + tmp_hour\n",
    "                tmp_dt = str(date_all[id_1]) + ' ' + tmp_hour\n",
    "                datetime_str.append(tmp_dt)\n",
    "            \n",
    "            '''\n",
    "            To complement data for no data some hours\n",
    "            '''\n",
    "            #To get start timestamp First \n",
    "            min_dt_str = datetime_str[0] + ':00:00'\n",
    "            min_tuple = time.strptime(min_dt_str, '%Y-%m-%d %H:00:00')\n",
    "            min_timestamp = time.mktime(min_tuple)\n",
    "            \n",
    "            #To get end timestamp \n",
    "            max_tuple = time.strptime(END_TIME[0:13] + \":00:00\", '%Y-%m-%d %H:00:00')\n",
    "            max_timestamp = time.mktime(max_tuple)\n",
    "           \n",
    "            hour_diff = int((max_timestamp - min_timestamp) / 3600)\n",
    "            all_db_util_list_1 = []\n",
    "            #clear date_all\n",
    "            date_all = []\n",
    "            #clear x_hour_list\n",
    "            x_hour_list = []\n",
    "            ori_dt_str = []\n",
    "            #Reassign three variables above\n",
    "            for id_2 in range(hour_diff):\n",
    "                tmp_st = min_timestamp + id_2 * 3600.0\n",
    "                #To change timestamp to datetime string\n",
    "                tmp_datetime = datetime.datetime.fromtimestamp(tmp_st)\n",
    "                tmp_datetime_str = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "                tmp_date_str = tmp_datetime_str[0:10]\n",
    "                tmp_hour_str = tmp_datetime_str[11:13]\n",
    "                ori_dt_str.append(tmp_date_str + ' ' + tmp_hour_str)\n",
    "                date_all.append(tmp_date_str)\n",
    "                x_hour_list.append(tmp_hour_str)\n",
    "\n",
    "                '''\n",
    "                Below code is for handling y-axis's data\n",
    "                If there is no data at this hour,0.0 will be filled into\n",
    "                '''\n",
    "                if tmp_datetime_str in ori_datetime_str:\n",
    "                    tmp_index = ori_datetime_str[tmp_datetime_str]\n",
    "                    all_db_util_list_1.append(y_db_util_percent_list[tmp_index])\n",
    "                else:\n",
    "                    all_db_util_list_1.append(1.0)\n",
    "            #all_db_util_list.append(all_db_util_list_1)       \n",
    "            df_final = pd.Series(all_db_util_list_1)\n",
    "            df_final.index = pd.PeriodIndex(start = ori_dt_str[0], end = ori_dt_str[len(ori_dt_str) - 1])\n",
    "            db_model = sm.tsa.ARIMA(df_final, (7,0,1)).fit()\n",
    "            \n",
    "            #To get end_timestamp from ori_dt_str for preparing to form x_ticks and x_ticks_lables\n",
    "            end_dt_str = ori_dt_str[len(ori_dt_str) - 1] + \":00:00\"\n",
    "            end_tuple = time.strptime(end_dt_str, '%Y-%m-%d %H:00:00')\n",
    "            end_timestamp = time.mktime(end_tuple)\n",
    "            \n",
    "            #The count of output original data \n",
    "            ori_hour = None\n",
    "            ori_hour_st = end_timestamp - 3600.0 * (INTERVAL - 1)\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(ori_hour_st)\n",
    "            ori_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "\n",
    "            #The count of predict data \n",
    "            predict_hour = None\n",
    "            predict_hour_st = None\n",
    "            #If INTERVAL>10，we will predict only 10 hours\n",
    "            #If not we we will predict INTERVAL hours\n",
    "            if(INTERVAL <= 10):\n",
    "                predict_hour_st = end_timestamp + 3600.0 * INTERVAL\n",
    "            else:\n",
    "                predict_hour_st = end_timestamp + 3600 * predict_count\n",
    "            tmp_datetime = datetime.datetime.fromtimestamp(predict_hour_st)\n",
    "            predict_hour = tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:13]\n",
    "            \n",
    "            #To get predict data\n",
    "            predict_array = db_model.predict(ori_dt_str[len(ori_dt_str) - 1], predict_hour, dynamic=True)\n",
    "            predict_array_list = list(predict_array)\n",
    "            #If predict_array including NaN value,change them into 0.0\n",
    "            for i in range(len(predict_array_list)):\n",
    "                if np.isnan(float(predict_array_list[i])):\n",
    "                    predict_array_list[i] = 0.0\n",
    "            #predict_array = np.asarray(predict_array_list[1:len(predict_array_list)])\n",
    "            all_db_util_list_1 = all_db_util_list_1[-INTERVAL:] + predict_array_list[1:len(predict_array_list)]\n",
    "            all_db_util_list.append(all_db_util_list_1)\n",
    "            \n",
    "            #To set x ticks and x label\n",
    "            loop_count = int((predict_hour_st - ori_hour_st)/3600.0)\n",
    "            date_all = []\n",
    "            x_hour = []\n",
    "            for id_3 in range(loop_count + 1):\n",
    "                tmp_hour_st = ori_hour_st + id_3 * 3600.0\n",
    "                tmp_datetime = datetime.datetime.fromtimestamp(tmp_hour_st)\n",
    "                date_all.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[0:10])\n",
    "                x_hour.append(tmp_datetime.strftime(\"%Y-%m-%d %H:00:00\")[11:13])\n",
    "            x_ticks = []\n",
    "            x_ticks_lables = []\n",
    "            #if len(x_ticks) == 0:\n",
    "            commonUtil.format_x_axis(date_all, x_hour, x_ticks, x_ticks_lables)\n",
    "        \n",
    "        if len(all_db_util_list) > 0:\n",
    "            #To declare color set\n",
    "            color_set = ['#ffb90f', '#a6266e', '#4c78fb', '#2c628b', '#60bdae']\n",
    "            #To declare a Sketchpad\n",
    "            fig = pl.figure()\n",
    "            #To declare an ax container as a drawing paper\n",
    "            ax_db_util= fig.add_subplot(111)\n",
    "\n",
    "            data_size = len(x_ticks)\n",
    "            if(data_size <= 20):\n",
    "                fig.set_size_inches(12,6)\n",
    "            elif(data_size <= 40):\n",
    "                fig.set_size_inches(16,6)\n",
    "            elif(data_size <= 60):\n",
    "                fig.set_size_inches(18,7)\n",
    "            elif(data_size <=100):\n",
    "                fig.set_size_inches(22,7)\n",
    "\n",
    "            #To set the title/label/grid for the graph\n",
    "            figure_title = 'The Max Database Usage by Hour\\n'\n",
    "            pl.title(figure_title, fontsize = 14, fontweight = 'bold')\n",
    "            x_lable = 'Hours'\n",
    "            #To set x-axis label\n",
    "            pl.xlabel(x_lable)\n",
    "            #To set y-axis label\n",
    "            pl.ylabel(u'DATABASE MAX_USAGE_GB')\n",
    "            #To set grid line style according to your requirement\n",
    "            pl.grid(True, ls = '--', color = '#2c628b', alpha = 0.05)\n",
    "            pl.xticks(x_ticks, x_ticks_lables, rotation = 90)\n",
    "        \n",
    "            #Below two variable is used for storeing magnified data\n",
    "            xnew_hour = []\n",
    "            ynew_db_util_percent = []\n",
    "        \n",
    "            for id_4 in range(len(all_db_util_list)):\n",
    "                #To scatter the data of y-axis and mark the data point\n",
    "                for id_5 in range(len(all_db_util_list[id_4])):\n",
    "                    if(all_db_util_list[id_4][id_5] == 0.0):#If no data,drawing a empty circle\n",
    "                        if id_5 >= len(all_db_util_list[id_4]) - predict_count and id_5 < len(all_db_util_list[id_4]):\n",
    "                            #This is the data of prediction and empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = 'r') \n",
    "                        else:\n",
    "                            #This is the real data of y_max_log_percent and empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = '', marker = 'o', edgecolors = 'r', s = 50)\n",
    "                    else:#Data not empty\n",
    "                        if id_5 >= len(all_db_util_list[id_4]) - predict_count and id_5 < len(all_db_util_list[id_4]):\n",
    "                            #This is the data of prediction and not empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = 'r') \n",
    "                            pl.text(x_ticks[id_5], all_db_util_list[id_4][id_5], '%.0f' % all_db_util_list[id_4][id_5], fontsize = 9)\n",
    "                        else:\n",
    "                            #This is the real data of y_max_log_percent and not empty\n",
    "                            pl.scatter(x_ticks[id_5], all_db_util_list[id_4][id_5], c = '#4c78fb') \n",
    "                            pl.text(x_ticks[id_5], all_db_util_list[id_4][id_5], '%.0f' % all_db_util_list[id_4][id_5], fontsize = 9)\n",
    "\n",
    "                #Expand each x axis data 20 times\n",
    "                xnew_hour = np.linspace(np.asarray(x_ticks).min(), np.asarray(x_ticks).max(), np.asarray(x_ticks).size*20) \n",
    "                #Handle the data of new y axis data\n",
    "                ynew_db_util_percent = spline(np.asarray(x_ticks), np.asarray(all_db_util_list[id_4]), xnew_hour)\n",
    "                ynew_db_util_percent_list = list(ynew_db_util_percent)\n",
    "                #No negative value for y-axis\n",
    "                for y_idx in range(len(ynew_db_util_percent_list)):\n",
    "                    if (ynew_db_util_percent_list[y_idx] < 0.0):\n",
    "                        ynew_db_util_percent_list[y_idx] = 0.0\n",
    "                ynew_db_util_percent = np.asarray(ynew_db_util_percent_list)\n",
    "                #Fill the gragh according to your requirement\n",
    "                db_ratio = len(list(predict_array)) / float('%.1f' % len(all_db_util_list[id_4]))\n",
    "                #pl.fill_between(xnew_hour, ynew_db_util_percent, where=(xnew_hour.min() < xnew_hour) & (xnew_hour < xnew_hour.max() * (1.0 - tbsp_ratio)), color = '#4c78fb', alpha = 0.15)\n",
    "                pl.fill_between(xnew_hour, ynew_db_util_percent, where=(xnew_hour.max() * (1.0 - db_ratio) < xnew_hour) & (xnew_hour < xnew_hour.max()), color = '#2c628b', alpha = 0.1)\n",
    "                #Draw curve graph\n",
    "                pl.plot(xnew_hour, ynew_db_util_percent, color = color_set[id_4])\n",
    "            #Set the legends for the both graphs\n",
    "            box_db = ax_db_util.get_position()\n",
    "            ax_db_util.set_position([box_db.x0, box_db.y0 + box_db.height * 0.1, box_db.width, box_db.height * 0.9])\n",
    "            ax_db_util.legend(legend_title_db, fontsize = 11, loc = 'upper center', bbox_to_anchor=(0.5,1.08), fancybox = True, shadow = True, ncol = len(legend_title_db))\n",
    "            #pl.yticks(np.arange(0, 101, 10))\n",
    "            pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
